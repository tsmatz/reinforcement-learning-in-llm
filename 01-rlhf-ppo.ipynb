{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "857cafc6-da38-4aa7-8afc-63aa626fa7aa",
   "metadata": {},
   "source": [
    "# RLHF (with PPO)\n",
    "\n",
    "In this tutorial, we'll apply RLHF (Reinforcement Learning from Human Feedback, [[Ouyang et al., 2022](https://arxiv.org/pdf/2203.02155)]) in binary preference.\n",
    "\n",
    "In RLHF (Reinforcement Learning from Human Feedback), the following 3 steps are conducted to finetune LLM and align human preference.\n",
    "\n",
    "- SFT (Supervised Fine-Tuning) : Firstly, LLM is trained to follow the supervised human dialogue dataset, with regular LLM training methods by minimizing cross-entropy loss for token ouputs. (See [here](https://github.com/tsmatz/nlp-tutorials/blob/master/09_transformer.ipynb) for regular LLM training.)\n",
    "- Build a Reward Model (RM) : Using the multiple outputs of LLM, the human annotation for comparison (such as, \"which answer is preferable, or not preferable\") is collected, and we then build a reward model to score the outputs using this annotation dataset.\n",
    "- Reward Training : With this generated reward model (RM), LLM is then trained by reinforcement learning method to maximize the reward score. In the paper, Proximal Policy Optimization (shortly, PPO) algorithm is conducted for RL method. (See [here](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb) for primitive outline about PPO.)\n",
    "\n",
    "In this tutorial, we use [SmolLM2-Instruct (135M)](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct) model (chat model) to train with existing human-preference dataset, [Human-Like-DPO-Dataset](https://huggingface.co/datasets/HumanLLMs/Human-Like-DPO-Dataset), instead of collecting annotations from humans by myself.\n",
    "\n",
    "> Note : SmolLM2-Instruct is also optimized by the following 2 phase fine-tuning.<br>\n",
    "> 1. Supervised fine-tuning (SFT) : The model generated by pretraining phase ([SmolLM2 (135M)](https://huggingface.co/HuggingFaceTB/SmolLM2-135M)) is fine-tuned to follow chat-styled conversation by supervised fine-tuning (SFT) using [SmolTalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk) dataset.\n",
    "> 2. Preference optimization : The model is then fine-tuned to generate the preferred outputs using [UltraFeedback](https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized) dataset. In SmolLM2-Instruct, DPO is applied in this phase.\n",
    "\n",
    "This dataset, [Human-Like-DPO-Dataset](https://huggingface.co/datasets/HumanLLMs/Human-Like-DPO-Dataset) is annotated to mimic human natural, informal, and conversational interaction.<br>\n",
    "Each line contains a pair of texts - one \"chosen\" and another \"rejected\". The chosen text is a preferred LLM output (more natural output) for humans, while the rejected one is a non-preferable output.\n",
    "\n",
    "In order to reduce complexity, we don't include the following training optimization in this tutorial. :\n",
    "\n",
    "- In the original [RLHF paper](https://arxiv.org/pdf/2203.02155), they have applied RLHF with ranked preference, but here (in this notebook) we'll apply RLHF with binary preference.\n",
    "- We don't also apply the training mixture with pretrained dataset. To mitigate performance regression, pretraining mixture is often applied in practice. (See [RLHF paper](https://arxiv.org/pdf/2203.02155) or [the paper](https://arxiv.org/pdf/2204.05862) by Anthropic.)\n",
    "\n",
    "<blockquote>\n",
    "Note : According to <a href=\"https://arxiv.org/pdf/2204.05862\">the paper</a> by Anthropic, they have conducted more meticulous RLHF steps as follows. (In this notebook, I have skipped all of these steps not to make things complex.)\n",
    "<ul>\n",
    "    <li>Context distillation by SFT often causes so called catastrophic forgetting, compared to the training by reinforcement learning. Not to make largely updated and overfitted, they have conducted distillation, decreasing KL divergence between base model and distilled model.</li>\n",
    "    <li>A reward model (LM for reward modeling) is initialized by a special method, which is the mixed training to reduce both autoregressive LM loss on good samples and regular preference modeling (PM) loss on entire samples.</li>\n",
    "    <li>To improve performance, they has also applied other methods - such as, adding \"end-of-context\" token at the end of each sample, etc.</li>\n",
    "</ul>\n",
    "</blockquote>\n",
    "\n",
    "Now let's see the code (implementation) step-by-step with description of theoretical aspects.\n",
    "\n",
    "Throughout this notebook, I have used pretrained model in Hugging Face, but **I manually configure and run training with regular PyTorch training loop** (i.e., won't use any built-in class in Hugging Face) not to make the implementation black-boxed.\n",
    "\n",
    "*(back to [index](https://github.com/tsmatz/reinforcement-learning-in-llm/))*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344efcc-98be-4f2e-8f6c-c355466a77da",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552467da-f1fc-452b-9684-4aa8c4665d09",
   "metadata": {},
   "source": [
    "Before we start, we need to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cd1d7-2218-42ec-acd1-84cccb566e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers datasets matplotlib pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fc9c0-a773-47e0-9b77-9149cb2d6926",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Before training a model, we should prepare (preprocess) the training dataset.\n",
    "\n",
    "In this example, we use dataset, [Human-Like-DPO-Dataset](https://huggingface.co/datasets/HumanLLMs/Human-Like-DPO-Dataset), for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c77faeaa-a81c-4bd8-b1ca-94eaee62fa3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'rejected'],\n",
       "    num_rows: 10884\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "all_data = load_dataset(\"HumanLLMs/Human-Like-DPO-Dataset\")\n",
    "train_data = all_data[\"train\"]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8a1afe-b83e-40aa-94bf-47902b9eb74f",
   "metadata": {},
   "source": [
    "This dataset has 2 labels, \"```chosen```\" and \"```rejected```\", which is respectively a preferred response (i.e., more natural and informal response) and a non-preferred response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b8cb328-765a-48f7-b07f-cf2311ba6414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** prompt **********\n",
      "Oh, I just saw the best meme - have you seen it?\n",
      "********** chosen **********\n",
      "ðŸ˜‚ Ah, no I haven't! I'm dying to know, what's the meme about? Is it a funny cat or a ridiculous situation? Spill the beans! ðŸ¤£\n",
      "********** rejected **********\n",
      "I'm an artificial intelligence language model, I don't have personal experiences or opinions. However, I can provide you with information on highly-rated and critically acclaimed films, as well as recommendations based on specific genres or themes. Would you like me to suggest some notable movies or discuss a particular genre of interest?\n",
      "********** end **********\n"
     ]
    }
   ],
   "source": [
    "row_num = 0\n",
    "print(\"********** prompt **********\")\n",
    "print(train_data[\"prompt\"][row_num])\n",
    "print(\"********** chosen **********\")\n",
    "print(train_data[\"chosen\"][row_num])\n",
    "print(\"********** rejected **********\")\n",
    "print(train_data[\"rejected\"][row_num])\n",
    "print(\"********** end **********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17e535-f24d-48b9-99fc-c72ca8d8c8da",
   "metadata": {},
   "source": [
    "Now we convert this data into the following text format, fitting for SmolLM2-Instruct inputs.<br>\n",
    "In this format, the user role in chat message is \"```<|im_start|>user\\n...<|im_end|>```\" and the assistant role in chat message is \"```<|im_start|>assistant\\n...<|im_end|>```\".\n",
    "\n",
    "```\n",
    "<|im_start|>user\n",
    "Oh, I just saw the best meme - have you seen it?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "ðŸ˜‚ Ah, no I haven't! I'm dying to know, what's the meme about? Is it a funny cat or a ridiculous situation? Spill the beans! ðŸ¤£<|im_end|>\n",
    "```\n",
    "\n",
    "> Note : If you want to include system role, use \"```<|im_start|>system\\n...<|im_end|>```\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9aa7c02a-060f-47ca-81b0-2469ad096987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_chatml(example):\n",
    "    return {\n",
    "        \"chosen\": f\"<|im_start|>user\\n{example[\"prompt\"]}<|im_end|>\\n<|im_start|>assistant\\n{example[\"chosen\"]}<|im_end|>\",\n",
    "        \"rejected\": f\"<|im_start|>user\\n{example[\"prompt\"]}<|im_end|>\\n<|im_start|>assistant\\n{example[\"rejected\"]}<|im_end|>\",\n",
    "    }\n",
    "\n",
    "original_columns = train_data.column_names\n",
    "train_data = train_data.map(format_to_chatml, remove_columns=original_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4185453d-acfe-42b8-9789-5b0801070b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** chosen **********\n",
      "<|im_start|>user\n",
      "Oh, I just saw the best meme - have you seen it?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "ðŸ˜‚ Ah, no I haven't! I'm dying to know, what's the meme about? Is it a funny cat or a ridiculous situation? Spill the beans! ðŸ¤£<|im_end|>\n",
      "********** rejected **********\n",
      "<|im_start|>user\n",
      "Oh, I just saw the best meme - have you seen it?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm an artificial intelligence language model, I don't have personal experiences or opinions. However, I can provide you with information on highly-rated and critically acclaimed films, as well as recommendations based on specific genres or themes. Would you like me to suggest some notable movies or discuss a particular genre of interest?<|im_end|>\n",
      "********** end **********\n"
     ]
    }
   ],
   "source": [
    "row_num = 0\n",
    "print(\"********** chosen **********\")\n",
    "print(train_data[\"chosen\"][row_num])\n",
    "print(\"********** rejected **********\")\n",
    "print(train_data[\"rejected\"][row_num])\n",
    "print(\"********** end **********\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae72f8d-4261-40b0-ad25-0877822a992e",
   "metadata": {},
   "source": [
    "## Run Supervised Fine-tuning (SFT)\n",
    "\n",
    "In original [RLHF](https://arxiv.org/pdf/2203.02155), the actual model's outputs are used to train model. In this example, however, we use the prepared dataset, which might be apart from the actual outputs induced by our model.<br>\n",
    "When the distribution for the outputs is extremely apart from objectives, it would then be so hard to bridge this gap, because only the actual outputs are used to seek the trajectory to the goal in reinforcement learning. (See [here](https://tsmatz.wordpress.com/2025/04/21/reinforcement-learning-for-llm/).)\n",
    "\n",
    "In this section, therefore, in order to keep the distribution of model's outputs within a certain range, we apply SFT, supervised fine-tuning (i.e., [imitation learning approach](https://tsmatz.wordpress.com/2025/04/21/reinforcement-learning-for-llm/)) using good samples, before running the optimization by reinforcement learning (RL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b420848-3c05-4dc8-87b7-aca5c8a7432c",
   "metadata": {},
   "source": [
    "Now we download [SmolLM2-Instruct (135M)](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct) model and its tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3de214-80a6-4a32-8a81-d81e42574d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "torch.set_default_dtype(torch.bfloat16) # because SmolLM2-Instruct is trained on bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e40e391-bd88-4e18-b4ff-f6a39c259cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "# download model\n",
    "config = AutoConfig.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    config=config,\n",
    ").to(device)\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d57a539-d269-45b4-abba-36ba6fe20224",
   "metadata": {},
   "source": [
    "We build dataloader in order to feed data to the trainer.<br>\n",
    "In SFT, we use only good samples (\"```chosen```\" tokens) and train on tokens of entire sequence, including both inputs and completions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fcd67d-2c9f-4fa1-a19a-82435be2dc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "def collate_batch(batch):\n",
    "    itr_batch_size = len(batch)\n",
    "\n",
    "    # tokenize (convert to token ids and attention mask) and convert to tensor\n",
    "    token_list = [item[\"chosen\"] for item in batch]\n",
    "    token_tensor = tokenizer(\n",
    "        token_list,\n",
    "        padding=True,\n",
    "        padding_side=\"right\",\n",
    "        return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # generate labels for SFT\n",
    "    labels = token_tensor[\"input_ids\"][:,1:].clone()\n",
    "    # generate inputs for SFT\n",
    "    last_nonpad_indices = token_tensor[\"attention_mask\"].sum(dim=1) - 1  # note: valid only in right padding\n",
    "    token_tensor[\"input_ids\"][torch.arange(itr_batch_size).to(device),last_nonpad_indices] = tokenizer.pad_token_id  # note: this is not needed, because the final token is always pad token\n",
    "    token_tensor[\"attention_mask\"][torch.arange(itr_batch_size).to(device),last_nonpad_indices] = 0\n",
    "    inputs = token_tensor[\"input_ids\"][:,:-1]\n",
    "    masks = token_tensor[\"attention_mask\"][:,:-1]\n",
    "\n",
    "    return inputs, labels, masks\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f3e31e-b0da-4d7e-8706-1f050dc7b9be",
   "metadata": {},
   "source": [
    "Now we train our base model.\n",
    "\n",
    "> Note : In order to prevent from GPU out of memory errors, I have used accumulation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5036c3bc-cfb5-4e90-b9a0-e3457414de3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (iter2721) 341/341 - loss 2.2344\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os, math\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import functools\n",
    "\n",
    "num_epochs = 1\n",
    "num_steps = math.ceil(len(dataloader) / gradient_accumulation_steps)\n",
    "\n",
    "# prepare optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=base_model.parameters(),\n",
    "    lr=9.0e-6,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    ")\n",
    "\n",
    "def _get_cosine_schedule(\n",
    "    current_step: int,\n",
    "    num_training_steps: int,\n",
    "    num_warmup_steps: int=0,\n",
    "    linear_warmup: bool=False,\n",
    "    min_value: float=0.0,\n",
    "):\n",
    "    if current_step < num_warmup_steps:\n",
    "        if linear_warmup:\n",
    "            return min(1.0, (current_step + 1) / (num_warmup_steps + 1))  # see https://arxiv.org/abs/2410.11020\n",
    "        else:\n",
    "            return 1.0\n",
    "    progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))\n",
    "    scale = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "    return (1.0 - min_value) * scale + min_value\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=functools.partial(\n",
    "    _get_cosine_schedule,\n",
    "    num_training_steps=num_epochs*num_steps,\n",
    "    min_value=0.3,\n",
    "))\n",
    "\n",
    "# remove log file if exists\n",
    "log_file = \"loss_sft.log\"\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# iterate epoch\n",
    "for epoch in range(num_epochs):\n",
    "    base_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    record_loss = []\n",
    "\n",
    "    # iterate batch\n",
    "    for i, (inputs, labels, masks) in enumerate(dataloader):\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # get logits and values to be optimized\n",
    "            outputs = base_model(\n",
    "                input_ids=inputs,\n",
    "                attention_mask=masks,\n",
    "            )\n",
    "\n",
    "            # compute loss\n",
    "            loss = F.cross_entropy(outputs.logits.transpose(1,2), labels)\n",
    "            record_loss.append(loss.item())\n",
    "\n",
    "            # optimize\n",
    "            loss.backward()\n",
    "            if ((i + 1) % gradient_accumulation_steps == 0) or \\\n",
    "               (i + 1 == len(dataloader)):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "            # print log\n",
    "            print(f\"Epoch {epoch+1} (iter{i+1}) {math.ceil((i + 1) / gradient_accumulation_steps)}/{num_steps} - loss {loss :5.4f}\", end=\"\\r\")\n",
    "\n",
    "    # save log in epoch\n",
    "    with open(log_file, \"a\") as f:\n",
    "        for l in record_loss:\n",
    "            f.write(\"%s\\n\" %l)\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "# save checkpoint\n",
    "### torch.save(base_model.state_dict(), \"llm_sft.pt\")\n",
    "base_model.save_pretrained(\"./llm_sft\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab8216-e860-4772-989f-ca7693eddf82",
   "metadata": {},
   "source": [
    "Show loss transition in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda0ffb8-6f5d-45a5-9812-379f23f01155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "log_file = \"loss_sft.log\"\n",
    "\n",
    "with open(log_file, \"r\") as f:\n",
    "    data = [float(line) for line in f]\n",
    "\n",
    "# show plot of every 50 interval average\n",
    "interval = 50\n",
    "avg = []\n",
    "for i in range(interval, len(data)):\n",
    "    tmp_list = data[i-interval+1:i+1]\n",
    "    avg.append(np.average(tmp_list))\n",
    "plt.plot(np.arange(interval, len(data)), avg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b41ac3-0b9d-45e3-9331-1d2883740eaa",
   "metadata": {},
   "source": [
    "## Test trained model (SFT-ed model)\n",
    "\n",
    "In order to test our fine-tuned model, we generate outputs for the following example questions.\n",
    "\n",
    "> Note : In this function (```generate_token_by_policy```), here we use **attention cache** for sequential token generation to speed up.<br>\n",
    "> And, in order to avoid GPU memory errors in optimization, we also limit the number of tokens in sequence (by ```max_seq_len``` parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c84cfbd-2c67-447a-8ebc-4daa354d5f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DynamicCache\n",
    "\n",
    "def generate_token_by_policy(\n",
    "    chat_data,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    max_seq_len,\n",
    "):\n",
    "    \"\"\"\n",
    "    Collect samples with a model (LLM) as a batch.\n",
    "    To speed up generation, here we use attention cache.\n",
    "    All tensors are collected with no gradient (as detached tensors).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chat_data : dic(\n",
    "            input_ids: torch.tensor((batch_size, seq_len), dtype=int),\n",
    "            attention_mask: torch.tensor((batch_size, seq_len), dtype=int)\n",
    "        )\n",
    "        Chat template data to be fed as a batch.\n",
    "        The format should be left-side padding, and shouldn't include the\n",
    "        final assistant's message, because it'll be generated in this function.\n",
    "        (The length of input's sequence (seq_len) might differ in each call.)\n",
    "    model : torch.nn.Module\n",
    "        A model which is used to pick up an action (i.e., a token).\n",
    "        In this function, the output is generated with no gradient.\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        Hugging Face tokenizer class to be used in this model.\n",
    "    max_seq_len : int\n",
    "        Maximum sequence length. (See above description.)\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    completion_ids : torch.tensor((batch_size, seq_len), dtype=int)\n",
    "        The array of token id for generated chat completion (including context tokens).\n",
    "        The length of result's sequence (i.e., seq_len) differs depending\n",
    "        on the results.\n",
    "    completion_mask : torch.tensor((batch_size, seq_len), dtype=int)\n",
    "        Corresponding attention mask.\n",
    "    \"\"\"\n",
    "\n",
    "    # get batch size\n",
    "    batch_size = chat_data[\"input_ids\"].shape[0]\n",
    "\n",
    "    # initialize inputs\n",
    "    cur_iids = chat_data[\"input_ids\"]\n",
    "    cur_mask = chat_data[\"attention_mask\"]\n",
    "\n",
    "    # initialize a flag for processing/finish in a batch\n",
    "    # (True: processing, False: finished)\n",
    "    proceed_flag = torch.ones(batch_size, dtype=bool).to(device)\n",
    "\n",
    "    # initialize cache parameters\n",
    "    cache_position = None\n",
    "    past_key_values = DynamicCache()\n",
    "\n",
    "    # loop until all is done\n",
    "    done_tokens_num = 0\n",
    "    while(torch.any(proceed_flag)):\n",
    "        # get current sequence length\n",
    "        cur_seq_len = cur_iids.shape[1]\n",
    "\n",
    "        # get the final non-pad token indices in sequence\n",
    "        # --> shape:[batch_size]\n",
    "        token_indices = torch.arange(cur_seq_len, dtype=int).to(device)\n",
    "        last_nonpad_indices = (token_indices * cur_mask).argmax(-1)\n",
    "\n",
    "        # run inference (with no gradient !)\n",
    "        if cache_position is None:\n",
    "            # get initial cache position\n",
    "            cache_position = torch.arange(cur_seq_len, dtype=int, device=device)\n",
    "            # compute logits for all input_ids\n",
    "            logits = model(\n",
    "                input_ids=cur_iids,\n",
    "                attention_mask=cur_mask,\n",
    "                cache_position=cache_position,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            ).logits.detach()\n",
    "            # need only final output in sequence --> shape:[batch_size, vocab_size]\n",
    "            logits = logits[torch.arange(batch_size).to(device), last_nonpad_indices, :]\n",
    "        else:\n",
    "            # compute logits only for the last input_ids\n",
    "            # (others are all cached.)\n",
    "            logits = model(\n",
    "                input_ids=cur_iids[:,-1:],\n",
    "                attention_mask=cur_mask,\n",
    "                cache_position=cache_position,\n",
    "                past_key_values=past_key_values,\n",
    "                use_cache=True,\n",
    "            ).logits.detach()\n",
    "            # reshape to [batch_size, vocab_size]\n",
    "            logits = logits.squeeze(1)\n",
    "\n",
    "        # select a token (i.e., take an action)\n",
    "        # --> shape:[batch_size]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        selected_ids = torch.multinomial(probs, num_samples=1).squeeze(-1)\n",
    "\n",
    "        # get next token indices in sequence\n",
    "        # --> shape:[batch_size]\n",
    "        next_token_indices = last_nonpad_indices + proceed_flag.int()\n",
    "\n",
    "        # expand inputs when it exceeds\n",
    "        # --> shape:[batch_size, cur_seq_len+1]\n",
    "        if next_token_indices.max() > cur_seq_len - 1:\n",
    "            cur_iids = F.pad(input=cur_iids, pad=(0, 1, 0, 0), mode=\"constant\", value=tokenizer.pad_token_id)\n",
    "            cur_mask = F.pad(input=cur_mask, pad=(0, 1, 0, 0), mode=\"constant\", value=0)\n",
    "\n",
    "        # store new token ids\n",
    "        cur_iids[proceed_flag, next_token_indices[proceed_flag]] = selected_ids[proceed_flag]\n",
    "\n",
    "        # store new attention mask\n",
    "        cur_mask[proceed_flag, next_token_indices[proceed_flag]] = 1\n",
    "\n",
    "        # update cache_position\n",
    "        cache_position = cache_position[-1:] + 1\n",
    "\n",
    "        # update proceed_flag\n",
    "        not_lim = (cur_mask.sum(dim=1) < max_seq_len)\n",
    "        is_eos = torch.logical_and((selected_ids == tokenizer.eos_token_id),proceed_flag.bool())\n",
    "        not_eos = torch.logical_not(is_eos)\n",
    "        proceed_flag = torch.logical_and(proceed_flag, torch.logical_and(not_lim, not_eos))\n",
    "\n",
    "    return cur_iids, cur_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fd5e6d6-293a-4d57-ab25-a382aceed7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Question *****\n",
      "What do you most want to do right now?\n",
      "***** Answer *****\n",
      "Where do I even start? I want to move on, to create something beautiful from scratch, something that will be a part of me for years to come. I envision a city on wheels, where every ray of light, every smile, and every crumpled up canvas flips the design over, and creates a message all its own.\n",
      "\n",
      "I want to create a mural, something that I can still take home years from now. I want the same colors, the same pattern, and no lines. My art will be unique, with a story behind it, and a story that starts from scratch in my mind.\n",
      "\n",
      "I also want to create a video game that responds to my emotions. I want characters that can see the light of day, identify with the gravity of the situation, and respond to my emotions in a way that feels almost organic.\n",
      "\n",
      "And, of course, there's the dreamy, beautiful landscapes of the city I'm creating. I want the buildings, the streets, the colors, and shapes to merge into one cohesive piece of art. I want to see art in my sleep, or through the eyes of a vintage clown in a time machine.\n",
      "\n",
      "I want to create something that is present in every walk of life, something that transcends borders, cultures, and ages. I want my work to reflect the diversity, the messiness, and the beauty of humanity.\n",
      "\n",
      "It's a daunting task, but I'm excited to take the leap and start building something truly magical.\n",
      "\n",
      "***** Question *****\n",
      "What is the best gift to give a friend who loves the outdoors?\n",
      "***** Answer *****\n",
      "To create a thoughtful and thoughtful gift for your friend who loves the outdoors, consider items that not only give them a unique experience but also reflect their personality and interests. For a casual getaway or weekend getaway, you can choose a decent-sized vehicle, nice looking roadside or recreational gear, and some outdoor games and equipment.\n",
      "\n",
      "However, if you have more limited collections or preferences, you could also think about gifts that are practical and make their trips enjoyable. For example, if they're interested in hiking gear or mapping a scenic route, you could pack a backpack with essentials and take some customized hiking boots.\n",
      "\n",
      "If they're into sports or adventure-driven activities, you could put on an outdoor clothing planner and book some recreational activities, such as fishing trips, fishing trips with campers, or camping excursions. If they're interested in learning about the local wildlife or experiencing a wildlife-friendly outdoor event, you could arrange for guided tours or excursions with professionals in their interests.\n",
      "\n",
      "Overall, a thoughtful gift that reflects their personality and preferences should not only accommodate their needs but also bring a sense of fun and enjoyment to your time together and create lasting memories.\n",
      "\n",
      "***** Question *****\n",
      "How do you relax after something bad happens?\n",
      "***** Answer *****\n",
      "Relaxation after a stressful experience is essential to prioritize your own well-being and maintain a positive mindset. One effective approach is to approach the situation with a sense of detachment and mindfulness. Allow yourself to breathe and slow down by letting go of any thoughts or emotions that might be contributing to your anxiety.\n",
      "\n",
      "Try to focus on the present moment, no matter how short it may seem, rather than dwelling on past regrets or planning solutions. Allow yourself to feel the stress and discomfort of the experience, releasing any physical tension or physical sensations that might raise concerns about your resilience.\n",
      "\n",
      "Consider activities that bring you joy, relaxation, or tranquility, such as taking a day trip, reading, or practicing deep breathing exercises. This can help distract your mind from the hindrance and cultivate a sense of flexibility and well-being.\n",
      "\n",
      "It's also essential to prioritize self-care, engaging in activities that promote physical and mental relaxation, and remembering why you stayed and acted when needed. This can be a great way to recharge and regain a sense of control over your situation, even if immediate relief doesn't arrive.\n",
      "\n",
      "Ultimately, relaxation is an ongoing process that requires patience, self-compassion, and self-awareness. By acknowledging that it's okay to feel uncomfortable or anxious, you can begin to make adjustments to ensure a more balanced and positive outlook from the start.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 768\n",
    "\n",
    "#\n",
    "# build a batch of questions\n",
    "# (To use cache, we apply left-side padding.)\n",
    "#\n",
    "\n",
    "messages = [\n",
    "    \"What do you most want to do right now?\",\n",
    "    \"What is the best gift to give a friend who loves the outdoors?\",\n",
    "    \"How do you relax after something bad happens?\",\n",
    "]\n",
    "inputs = [f\"<|im_start|>user\\n{m}<|im_end|>\\n<|im_start|>assistant\\n\" for m in messages]\n",
    "input_batch = tokenizer(\n",
    "    inputs,\n",
    "    padding=True,\n",
    "    padding_side=\"left\",\n",
    "    return_tensors=\"pt\").to(device)\n",
    "input_seq_len = input_batch[\"input_ids\"].shape[1]\n",
    "\n",
    "#\n",
    "# generate model's outputs\n",
    "#\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"./llm_sft\").to(device)\n",
    "base_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    iids, mask = generate_token_by_policy(\n",
    "        input_batch,\n",
    "        base_model,\n",
    "        tokenizer,\n",
    "        max_seq_len,\n",
    "    )\n",
    "iids = iids[:,input_seq_len:]\n",
    "outputs = tokenizer.batch_decode(iids, skip_special_tokens=True)\n",
    "\n",
    "#\n",
    "# print results\n",
    "#\n",
    "\n",
    "for i in range(len(messages)):\n",
    "    print(\"***** Question *****\")\n",
    "    print(messages[i])\n",
    "    print(\"***** Answer *****\")\n",
    "    print(outputs[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936cf380-a980-4942-bff1-2d86ec58d176",
   "metadata": {},
   "source": [
    "## Build a Reward Model (RM)\n",
    "\n",
    "In reward modeling, we build a model to score outputs.\n",
    "\n",
    "In this phase, language model (in this case, SmolLM2-Instruct) is usually used as a reward model by changing the modelâ€™s output layer from token probabilities to scalar value (reward value), because the language model has much abilities to capture language properties. (In original [RLHF paper](https://arxiv.org/pdf/2203.02155), GPT is also used for a reward model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0908b9e-f663-440e-91dd-c99b51039670",
   "metadata": {},
   "source": [
    "### 1. Define a Reward Model\n",
    "\n",
    "In this example, we also use fine-tuned SmolLM2-Instruct chat model for reward modeling.<br>\n",
    "Firstly I'll load the fine-tuned model (which is built in the previous \"SFT\" section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1380a440-7b6f-492c-998c-a68154b0ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\"./llm_sft\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efa4118-517e-48b8-949b-adee72253a5a",
   "metadata": {},
   "source": [
    "SmolLM2-Instruct (135M) is based on Meta Llama and it has 30 layers in autoregressive Transformer. (See below picture.)\n",
    "\n",
    "![SmolLM2 model (Llama)](./assets/rlhf_llama2.png)\n",
    "\n",
    "> Note : In this architecture, the positional embedding for query and key is Rotary positional embedding.<br>\n",
    "> See [here](https://github.com/meta-llama/llama/blob/llama_v2/llama/model.py) for the source code of Llama model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "51a0b79d-321e-47f8-a369-cc7b0d861822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bc7b1-a7a8-4ffc-8657-49b22ef3b5df",
   "metadata": {},
   "source": [
    "The final linear layer (named \"```lm_head```\" in above) has the hidden features (576 tensors) as input, and the logits for all 49152 vocabularies (all 49152 tokens) as output.\n",
    "\n",
    "In reward modeling (RM),\n",
    "\n",
    "- We should change the final hidden linear (named \"```lm_head```\") to output a reward score (i.e., a single tensor).\n",
    "- We then need a score **only in final token**. (Scores in other positions are disposed.)\n",
    "\n",
    "![Reward model (RM)](./assets/rlhf_reward_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "421981ed-b602-4f2c-b35d-c73634d705e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# define model class\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # replace final linear layer\n",
    "        self.base_model = base_model\n",
    "        self.base_model.__setattr__(\n",
    "            \"lm_head\",\n",
    "            nn.Linear(576, 1, bias=False).to(device))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "        # need only final output in sequence\n",
    "        batch_size = input_ids.shape[0]\n",
    "        token_indices = torch.arange(input_ids.shape[-1], dtype=int).to(device)\n",
    "        last_nonpad_indices = (token_indices * attention_mask).argmax(-1)\n",
    "        reward_score = output[torch.arange(batch_size).to(device), last_nonpad_indices]\n",
    "\n",
    "        return reward_score\n",
    "\n",
    "# generate model instance\n",
    "rm = RewardModel(base_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdd9a39f-25b2-4e73-9079-7adf870adeff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardModel(\n",
       "  (base_model): LlamaForCausalLM(\n",
       "    (model): LlamaModel(\n",
       "      (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "      (layers): ModuleList(\n",
       "        (0-29): 30 x LlamaDecoderLayer(\n",
       "          (self_attn): LlamaAttention(\n",
       "            (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "            (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "            (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "            (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          )\n",
       "          (mlp): LlamaMLP(\n",
       "            (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "            (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "            (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "          (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        )\n",
       "      )\n",
       "      (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      (rotary_emb): LlamaRotaryEmbedding()\n",
       "    )\n",
       "    (lm_head): Linear(in_features=576, out_features=1, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show how it's changes\n",
    "rm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0bccb8-93d3-4439-ac30-d1a524f81a57",
   "metadata": {},
   "source": [
    "### 2. Filter dataset\n",
    "\n",
    "RLHF is resource-intensive, because it simultaneously needs multiple optimization during training.<br>\n",
    "In order to reduce memory consumption (prevent from GPU out of memory errors), we now filter data not to exceed a certain length of tokens.\n",
    "\n",
    "> Note : In my experiment, I have used NVIDIA Tesla T4. Depending on your environment, you can increase the maximum sequence length.<br>\n",
    "> The operable maximum sequence length in ```SmolLM2-Instruct 135M``` is 8192. (See ```tokenizer.model_max_length```.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c37804b7-3b48-4490-b6e7-9eaed72bb76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected'],\n",
       "    num_rows: 10820\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. add \"chosen_len\" and \"rejected_len\" column\n",
    "#   (which indicates the sequence length in \"chosen\" and \"rejected\")\n",
    "def add_seq_len(example):\n",
    "    def get_tokenized_length(text):\n",
    "        tokenized = tokenizer(text)\n",
    "        return len(tokenized[\"input_ids\"])\n",
    "\n",
    "    chosen_len = get_tokenized_length(example[\"chosen\"])\n",
    "    reject_len = get_tokenized_length(example[\"rejected\"])\n",
    "    return {\n",
    "        \"chosen_len\": chosen_len,\n",
    "        \"rejected_len\": reject_len\n",
    "    }\n",
    "\n",
    "train_data = train_data.map(add_seq_len)\n",
    "\n",
    "# 2. remove rows which exceed the maximum sequence length\n",
    "train_data = train_data.filter(lambda example: example[\"chosen_len\"] <= max_seq_len and example[\"rejected_len\"] <= max_seq_len)\n",
    "train_data = train_data.remove_columns([\"chosen_len\", \"rejected_len\"])\n",
    "\n",
    "# show total number of filtered rows\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ccf79e-2c53-4967-a419-eb4e17a1c3cb",
   "metadata": {},
   "source": [
    "### 3. Train a reward model\n",
    "\n",
    "Now let's train a reward model with above data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaac50e1-3890-47ba-bb17-8af107c280f2",
   "metadata": {},
   "source": [
    "First we build dataloader in order to feed data to the trainer.\n",
    "\n",
    "Each input's batch is tokenized as PyTorch tensors on GPU.\n",
    "\n",
    "It's worth noting that, in Llama family, we can use **left-side padding** for input's tokenization. Throughout our training, we then also use left-side padding.\n",
    "\n",
    "> Note : In PPO training (in the next phase), we'll use attention cache to speed up training, and left-side padding is so useful. For this reason, we also choose the left-side padding in RM training. (The output's result will slightly differ between right-side padding and left-side padding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4968285a-7df8-4d84-abe9-480420a5c9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # tokenize (convert to token ids and attention mask) and convert to tensor\n",
    "    chosen_list = [item[\"chosen\"] for item in batch]\n",
    "    reject_list = [item[\"rejected\"] for item in batch]\n",
    "    chosen_tensor = tokenizer(\n",
    "        chosen_list,\n",
    "        padding=True,\n",
    "        padding_side=\"left\",  # see above description !\n",
    "        return_tensors=\"pt\").to(device)\n",
    "    reject_tensor = tokenizer(\n",
    "        reject_list,\n",
    "        padding=True,\n",
    "        padding_side=\"left\",  # see above description !\n",
    "        return_tensors=\"pt\").to(device)\n",
    "    return chosen_tensor, reject_tensor\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca8b628-af8e-459a-aa38-927cfd1fae90",
   "metadata": {},
   "source": [
    "Now we optimize (train) a reward model.\n",
    "\n",
    "According to [Bradleyâ€“Terry model](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model), when I denote the input context (prompt) as $x$ and the output completion as $y$, the probability $p(y_1 \\succ y_2 | x)$ which means \"how preferable the output $y_1$ is more than the output $y_2$ for the input $x$\" is represented by the following equation. :\n",
    "\n",
    "$\\displaystyle p(y_1 \\succ y_2 | x) = \\frac{\\exp(r(x, y_1))}{\\exp(r(x, y_1)) + \\exp(r(x, y_2))} \\;\\;\\;\\;\\;\\; (1)$\n",
    "\n",
    "where $r(\\cdot)$ is a reward function.\n",
    "\n",
    "> Note : For multiple ranked answers (not binary preference), [Plackett-Luce ranking model](https://doi.org/10.2307/2346567) can be used.\n",
    "\n",
    "It's worth noting that it always satisfies $ 0 \\leq p(y_1 \\succ y_2 | x) \\leq 1 $. (The preference should not be a negative number and not larger than 1.)\n",
    "\n",
    "> Note : In this equation, $\\exp(r)$ represents a preference score, in which $\\exp(r) = 1$ (i.e., $r = 0$) means that it's neutral, and $\\exp(r) \\approxeq 0$ (i.e., $r \\ll 0$) means that it's not preferable at all. (When it's preferable, $\\exp(r)$ is larger than 1.)<br>\n",
    "> In Bradleyâ€“Terry equation $p_1 / (p_1 + p_2)$, both $p_1$ and $p_2$ should be a positive real-valued score, in which $p=1$ (not $p=0$) is neutral. On the contrary, the reward $r$ can be positive and negative, in which the neutral is zero.<br>\n",
    "> Given partial rewards $r_1$ and $r_2$, the total preference score should be $p_1 \\cdot p_2 = \\exp(r_1) \\cdot \\exp(r_2) = \\exp(r_1 + r_2)$, which means that the total reward is $r_1 + r_2$.\n",
    "\n",
    "In order to maximize (1), now we apply a logarithm for the equation (1), and we then obtain the following preference modeling loss for each pairs of good and bad samples, $y_1$ and $y_2$. (When it's not trained at all, the mean of loss becomes ```log2 = 0.6931```.)\n",
    "\n",
    "$\\displaystyle L_{PM} = \\log \\left( 1 + \\frac{\\exp(r(x, y_2))}{\\exp(r(x, y_1))} \\right) \\;\\;\\;\\;\\;\\; (2)$\n",
    "\n",
    "Our goal of optimization is then to minimize the loss (2).\n",
    "\n",
    "It's worth noting that the loss value (2) becomes more smaller when the magnitude of rewards becomes greater. (When $ e^{r_2}/e^{r_1} \\lt 1$, then $ e^{r_2 \\cdot m}/e^{r_1 \\cdot m} = (e^{r_2}/e^{r_1})^m \\approxeq 0$ for arbitrary large $ m \\gg 1$.)<br>\n",
    "In [OpenAI implementation](https://github.com/openai/lm-human-preferences), they're adjusting a reward (which is produced by the trained model) to be normalized by $\\mathcal{N}(0, 1)$ in the inference. There also exists a method in the training to give incentives to force the mean to 0.<br>\n",
    "In this example, however, we don't take any adjustments for this concern.\n",
    "\n",
    "We should also take care for [overfitting](https://tsmatz.wordpress.com/2017/09/13/overfitting-for-regression-and-deep-learning/) problems during the training. (Train with a lot variety of data and less epochs.)\n",
    "\n",
    "> Note : In order to prevent from GPU out of memory errors, I have used accumulation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b351dfb9-94e0-415f-a7bf-d0d36b8f10f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (iter2705) 339/339 - loss 0.0464\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "num_steps = math.ceil(len(dataloader) / gradient_accumulation_steps)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    params=rm.parameters(),\n",
    "    lr=3.0e-5,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    ")\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=functools.partial(\n",
    "    _get_cosine_schedule,\n",
    "    num_training_steps=num_epochs*num_steps,\n",
    "    num_warmup_steps=math.ceil(num_epochs*num_steps*0.1)))\n",
    "\n",
    "# remove log file if exists\n",
    "log_file = \"loss_rm.log\"\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# iterate epoch\n",
    "for epoch in range(num_epochs):\n",
    "    rm.train()\n",
    "    optimizer.zero_grad()\n",
    "    record_loss = []\n",
    "\n",
    "    # iterate batch\n",
    "    for i, (chosen, reject) in enumerate(dataloader):\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # get reward score (chosen)\n",
    "            rewards_chosen = rm(\n",
    "                input_ids=chosen[\"input_ids\"],\n",
    "                attention_mask=chosen[\"attention_mask\"],\n",
    "            )\n",
    "            # get reward score (rejected)\n",
    "            rewards_reject = rm(\n",
    "                input_ids=reject[\"input_ids\"],\n",
    "                attention_mask=reject[\"attention_mask\"],\n",
    "            )\n",
    "            # compute loss\n",
    "            loss = -F.logsigmoid(rewards_chosen - rewards_reject).mean()\n",
    "            loss.backward()\n",
    "            # optimization by accumulation\n",
    "            if ((i + 1) % gradient_accumulation_steps == 0) or \\\n",
    "               (i + 1 == len(dataloader)):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            # print log\n",
    "            record_loss.append(loss.item())\n",
    "            print(f\"Epoch {epoch+1} (iter{i+1}) {math.ceil((i + 1) / gradient_accumulation_steps)}/{num_steps} - loss {loss :5.4f}\", end=\"\\r\")\n",
    "\n",
    "    # save logging\n",
    "    epoch_average_loss = sum(record_loss)/len(record_loss)\n",
    "    print(f\"Epoch {epoch+1} (iter{i+1}) {math.ceil((i + 1) / gradient_accumulation_steps)}/{num_steps} - loss {epoch_average_loss :5.4f}\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        for l in record_loss:\n",
    "            f.write(\"%s\\n\" %l)\n",
    "\n",
    "# save checkpoint\n",
    "torch.save(rm.state_dict(), \"rm.pt\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8108697-7dd2-42a4-aa04-4eae3a87a692",
   "metadata": {},
   "source": [
    "Show loss transition in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a8b2c52-d57e-4ec3-af69-f6ec157c2201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log_file = \"loss_rm.log\"\n",
    "\n",
    "with open(log_file, \"r\") as f:\n",
    "    data = [float(line) for line in f]\n",
    "\n",
    "plt.plot(np.arange(len(data)), data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca1d9f3-4f9d-4222-b3ab-eb9f5d63eec6",
   "metadata": {},
   "source": [
    "See reward scores of the following example text with a trained reward model (RM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "425f87ed-e1df-46e8-b1ab-fdb660f20cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|im_start|&gt;user\\nWhat do you most want to do ...</td>\n",
       "      <td>1.070312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|im_start|&gt;user\\nWhat do you most want to do ...</td>\n",
       "      <td>5.656250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|im_start|&gt;user\\nWhat is the best gift to giv...</td>\n",
       "      <td>1.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|im_start|&gt;user\\nWhat is the best gift to giv...</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     score\n",
       "0  <|im_start|>user\\nWhat do you most want to do ...  1.070312\n",
       "1  <|im_start|>user\\nWhat do you most want to do ...  5.656250\n",
       "2  <|im_start|>user\\nWhat is the best gift to giv...  1.265625\n",
       "3  <|im_start|>user\\nWhat is the best gift to giv...  4.250000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# # uncomment when you load RM from the trained checkpoint\n",
    "# rm.load_state_dict(torch.load(\"rm.pt\", weights_only=True))\n",
    "\n",
    "rm.eval()\n",
    "\n",
    "test_list = [\n",
    "    \"<|im_start|>user\\nWhat do you most want to do right now?<|im_end|>\\n<|im_start|>assistant\\nRight now, I most want to assist you and provide helpful, respectful, and engaging interactions. If you have any questions, need information, or just want to chat, I'm here for you!<|im_end|>\",\n",
    "    \"<|im_start|>user\\nWhat do you most want to do right now?<|im_end|>\\n<|im_start|>assistant\\nI'd love to go for a hike in the woods! ðŸ˜ It's such a beautiful day and I really need to get some fresh air. How about you? ðŸ˜Š<|im_end|>\",\n",
    "    \"<|im_start|>user\\nWhat is the best gift to give a friend who loves the outdoors?<|im_end|>\\n<|im_start|>assistant\\nThat's a thoughtful question! Here are a few gift ideas for a friend who loves the outdoors.: High-quality Multi-tool, Reusable Water Bottle with Filter, or Portable Hammock<|im_end|>\",\n",
    "    \"<|im_start|>user\\nWhat is the best gift to give a friend who loves the outdoors?<|im_end|>\\n<|im_start|>assistant\\nOoh, that's a fun question!! ðŸ™‹ If you're looking for some gift ideas for a friend who loves the outdoors, I suggest National Parks Pass, Hammock, or Portable Camping Chair! ðŸ¤ªðŸ¤£<|im_end|>\",\n",
    "]\n",
    "test_batch = tokenizer(\n",
    "    test_list,\n",
    "    padding=True,\n",
    "    padding_side=\"left\",\n",
    "    return_tensors=\"pt\").to(device)\n",
    "test_rewards = rm(\n",
    "    input_ids=test_batch[\"input_ids\"],\n",
    "    attention_mask=test_batch[\"attention_mask\"],\n",
    ")\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"text\": test_list,\n",
    "    \"score\": test_rewards.squeeze(-1).tolist()\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d818cd7-5219-4b71-9645-724f9d20d33a",
   "metadata": {},
   "source": [
    "Here we have trained a reward model (RM) from the beginning by ourselves, but **there already exist many open source pre-trained reward models available**. (You can also bring these pre-trained models in the following LLM training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd27007d-1328-4780-9fd9-de305d0d9e40",
   "metadata": {},
   "source": [
    "## LLM Reward Training (with PPO)\n",
    "\n",
    "Using our trained reward model, now let's train LLM to generate preferred outputs.\n",
    "\n",
    "Reinforcement learning (RL) is a machine learning method where the agent learns to maximize the obtained rewards, and in this example, LLM itself is an agent to be learnt. (Here I don't go so far, but please see [here](https://tsmatz.wordpress.com/2025/04/21/reinforcement-learning-for-llm/) for the paradigm of reinforcement learning adoption in LLM training.)\n",
    "\n",
    "PPO (Proximal Policy Optimization) is one of famous and state-of-the-art (SOTA) algorithms in reinforcement learning (RL), in which the agent learns optimal actions (which maximize rewards) with stable updates by evaluating KL divergence during the training.\n",
    "\n",
    "> Note : PPO (Proximal Policy Optimization) is a method developed by OpenAI.\n",
    "\n",
    "PPO requires 2 models - policy model (actor) and value model (critic). In LLM alignment, the policy model (actor) generates logit's outputs for tokens, and the value model (critic) generates a single scalar value.<br>\n",
    "Both policy model (actor) and value model (critic) reuse the pre-trained LLM to get optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad8433-9f22-4dc5-945d-c44780d710bf",
   "metadata": {},
   "source": [
    "### 1. Load a policy model (actor model)\n",
    "\n",
    "For a policy model (actor), I load fine-tuned LLM (SFT model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec5633cd-1d53-4d2b-9589-89c7bb123535",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = AutoModelForCausalLM.from_pretrained(\"./llm_sft\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f963da0-a774-4e34-8684-237906ab784c",
   "metadata": {},
   "source": [
    "### 2. Load a value model (critic model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123877fd-a743-4060-8ca1-e8f38b451691",
   "metadata": {},
   "source": [
    "A value model (critic model) is also based on language model, but it generates a scalar value as an output, such like a previous reward model. But a value model returns values on all token positions (not only the last token).\n",
    "\n",
    "> Note : In OpenAI [RLHF example implementation](https://github.com/openai/lm-human-preferences), a policy model and a value model share the weights each other to reduce GPU memory consumption. In order to get better performance, however, I separate policy model (actor) and value model (critic) in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b92cd1c3-75bb-4711-8d8b-84ba75cbd8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # replace final linear layer\n",
    "        self.base_model = base_model\n",
    "        self.base_model.__setattr__(\n",
    "            \"lm_head\",\n",
    "            nn.Linear(576, 1, bias=False).to(device))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"./llm_sft\").to(device)\n",
    "value_model = ValueModel(base_model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043acbbd-a25b-41ef-b8a5-1b854ffbea29",
   "metadata": {},
   "source": [
    "### 3. Create dataset\n",
    "\n",
    "Next we create a dataset for trainer.\n",
    "\n",
    "Here we also use previous chat dataset, but the final message (which is the reply message of assistant's role) is not required in this training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba1e9072-14fe-4b9e-87ba-fc05338a0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the final message,\n",
    "# but it keeps the final \"<|assistant|>\\n\"\n",
    "def rm_fin_msg(chat_str):\n",
    "    target = \"<|im_start|>assistant\\n\"\n",
    "    start_idx = chat_str.rfind(target)\n",
    "    return chat_str[:(start_idx + len(target))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e45c692a-f28d-44b8-a609-dddb7e49ce7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** original chat **********\n",
      "<|im_start|>user\n",
      "Oh, I just saw the best meme - have you seen it?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "ðŸ˜‚ Ah, no I haven't! I'm dying to know, what's the meme about? Is it a funny cat or a ridiculous situation? Spill the beans! ðŸ¤£<|im_end|>\n",
      "********** converted chat **********\n",
      "<|im_start|>user\n",
      "Oh, I just saw the best meme - have you seen it?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "********** original chat **********\n",
      "<|im_start|>user\n",
      "Do you have a go-to karaoke jam?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Oh, totally! ðŸ˜„ I'm a sucker for a good ol' rock ballad. Give me some Bon Jovi any day of the week! \"Livin' on a Prayer\" is my go-to karaoke jam. There's just something about belting out \"Oh, we're halfway there!\" at the top of my lungs that gets me pumped up! ðŸŽ¤ What about you, do you have a favorite karaoke song? ðŸŽ¶<|im_end|>\n",
      "********** converted chat **********\n",
      "<|im_start|>user\n",
      "Do you have a go-to karaoke jam?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "********** original chat **********\n",
      "<|im_start|>user\n",
      "**Crafty corner** Are you good at any DIY projects or crafts?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "ðŸ˜Š I'm actually a big fan of DIY projects! I'm not super skilled, but I love trying out new things and getting creative. I've tried my hand at painting, making jewelry, and even some woodworking (with lots of supervision, of course ðŸ˜‰). One time, I even attempted to make my own candles, and let's just say it was... an experience. The result was more \"rustic\" than \"refined,\" but it was fun!\n",
      "\n",
      "What about you? Are you crafty? Do you have a favorite DIY project or craft that you enjoy working on? ðŸŽ¨ðŸ’¡<|im_end|>\n",
      "********** converted chat **********\n",
      "<|im_start|>user\n",
      "**Crafty corner** Are you good at any DIY projects or crafts?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in range(3):\n",
    "    print(\"********** original chat **********\")\n",
    "    print(train_data[\"chosen\"][row])\n",
    "    print(\"********** converted chat **********\")\n",
    "    print(rm_fin_msg(train_data[\"chosen\"][row]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaffd825-81d4-494c-b827-106a283050b6",
   "metadata": {},
   "source": [
    "It's worth noting that, in order to use caching (for speed-up), we also need **left-side padding** in the training batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5748ab8-6e75-49d6-9176-ea4e5bf7f4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "gradient_accumulation_steps = 8\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # remove the final message\n",
    "    chat_list = [rm_fin_msg(item[\"chosen\"]) for item in batch]\n",
    "\n",
    "    # tokenize (convert to token ids and attention mask) and convert to tensor\n",
    "    chat_tensor = tokenizer(\n",
    "        chat_list,\n",
    "        padding=True,\n",
    "        padding_side=\"left\",\n",
    "        return_tensors=\"pt\").to(device)\n",
    "    return chat_tensor\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba621900-daf7-46f9-b7a7-70b0f40a3f83",
   "metadata": {},
   "source": [
    "### 4. Train with PPO\n",
    "\n",
    "Now let's train model with reinforcement learning.<br>\n",
    "During the training, policy model's outputs (i.e., LLM's outputs) are collected in order to feed them into trainer.\n",
    "\n",
    "Firstly, I'll summarize the brief outline of PPO algorithm.<br>\n",
    "For primitive outline about PPO, see [this tutorial](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb), in which I briefly (shortly) explain the theoretical background of PPO for beginners.\n",
    "\n",
    "In PPO, the objective is to minimize policy loss $L_P$ and value loss $L_V$.<br>\n",
    "Unlike [this beginner's tutorial](https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb), here I separately (not simultaneously) train $L_P$ and $L_V$ with multiple optimizers, in order for reducing memory consumption (preventing from GPU out-of-memory error).\n",
    "\n",
    "The policy loss $L_P$ is basically given by the following equation.\n",
    "\n",
    "$\\displaystyle L_P = L_{\\verb|ADV|} + \\beta \\cdot \\verb|KL| \\left( P(\\cdot | \\pi_{\\theta_{old}}(s)) \\| P(\\cdot | \\pi_{\\theta_{new}}(s)) \\right) $\n",
    "\n",
    "where\n",
    "\n",
    "$\\displaystyle L_{\\verb|ADV|} = (-1) \\frac{P(a | \\pi_{\\theta_{new}}(s))}{P(a | \\pi_{\\theta_{old}}(s))} A(a) $\n",
    "\n",
    "and\n",
    "\n",
    "- $\\pi_{\\theta}(\\cdot)$ is a policy, and $\\theta$ is parameters in policy model.\n",
    "- $P(a  | \\pi_{\\theta}(s))$ is a probability of taking action $a$ in the state $s$ over the policy $\\pi_{\\theta}(\\cdot)$. (In LLM, $a$ is a selected token, and $s$ is current token sequence.)\n",
    "- $A(a)$ is the advantage of taking action $a$. I'll explain how to compute $A(a)$ in below. When it takes a large cumulative rewards compared with the estimated one by taking action $a$, then $A(a)$ becomes positively large. When it takes a small cumulative rewards compared with the estimated one, then $A(a)$ becomes negatively large.\n",
    "- $\\verb|KL| ( P \\| Q )$ is KL divergence between the distibution $P$ and $Q$. In PPO, this term is used to prevent from large updates. When $P$ and $Q$ are the exact same distribution,  $\\verb|KL| ( P \\| Q ) = 0$. Otherwise,  $\\verb|KL| ( P \\| Q )$ becomes positive.\n",
    "- A loss coefficient $\\beta$ is a training parameter (hyperparameter), which represents the ratio of KL divergence loss.\n",
    "\n",
    "In categorical disribution, the log probability $\\log P(a)$ can be derived by the negative value of cross-entropy error - i.e., ```-torch.nn.functional.cross_entropy(l, a)```, where ```l``` is logits and ```a``` is a taken action (i.e., a selected token).\n",
    "\n",
    "KL divergence $\\verb|KL| \\left( P(\\cdot | \\pi_{\\theta_{old}}(s)) \\| P(\\cdot | \\pi_{\\theta_{new}}(s)) \\right)$ can be obtained by the following equation.\n",
    "\n",
    "$\\displaystyle \\verb|KL| \\left( P(\\cdot | \\pi_{\\theta_{old}}(s)) \\| P(\\cdot | \\pi_{\\theta_{new}}(s)) \\right) = -\\sum_a \\left( P(a|\\pi_{\\theta_{old}}(s)) \\log{\\frac{P(a|\\pi_{\\theta_{new}}(s))}{P(a|\\pi_{\\theta_{old}}(s))}} \\right)$\n",
    "\n",
    "> Note : This computation (KL computation) will need a lot of computing resources especially in language models, because it has a large number of vocabularies (i.e., candidate actions $a$).<br>\n",
    "> In practical training, therefore, $\\mathbb{E}_{(s,a)}\\left[\\log P(a|\\pi_{\\theta_{old}}(s)) - \\log P(a|\\pi_{\\theta_{new}}(s))\\right]$ is often used as KL divergence, instead of computing for all possible $a$, because KL divergence is interpreted as $\\mathbb{E}_{(s,a)}\\left[\\log P(a|\\pi_{\\theta_{old}}(s)) - \\log P(a|\\pi_{\\theta_{new}}(s))\\right]$ when the number of collected pairs $(s, a)$ is so large.<br>\n",
    "> In this example, we compute above equation (summation for all possible tokens).\n",
    "\n",
    "Value loss $L_V$, on the other hand, is basically obtained as a mean square loss (MSE) of difference between estimated values and actual values $V^{\\verb|ACTUAL|}(s_t) = \\sum_{l=0}^{\\infty} \\gamma^l r_{t + l}$. (Unlike advantages, value loss is therefore always positive.)\n",
    "\n",
    "$\\displaystyle L_V = \\frac{1}{N} \\sum_{n=1}^N \\frac{\\| V(s) - V^{\\verb|ACTUAL|}(s) \\|^2}{2}$\n",
    "\n",
    "where $V(s)$ is a value model and $N$ is a training batch size.\n",
    "\n",
    "To get better performance, here I change this basic formula as follows. (Now I'll explain how each components are optimized.)<br>\n",
    "\n",
    "**Advatnage**\n",
    "\n",
    "To get the advantage $A(a)$, we'll take TD (temporal difference) approach and use GAE (generalized advantage estimation) instead of regular advantages. (See [[Schulman, et al., 2018](https://arxiv.org/pdf/1506.02438)].)<br>\n",
    "First we can get the actual value at time-step $t$ by $r_t + \\gamma V(s_{t+1})$, where $r_t$ is an actual reward at time-step $t$ and $\\gamma$ is a discount rate. (In this example, we'll set $\\gamma=1.0$.)<br>\n",
    "We, therefore, get the difference $\\delta_t^V$ between actual value and estimated value, by the following equation :\n",
    "\n",
    "$\\displaystyle \\delta_t \\coloneqq r_t + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "\n",
    "In GAE (generalized advantage estimation), the advantage at $t$ is defined by :\n",
    "\n",
    "$\\displaystyle \\hat{A}_t^{\\verb|GAE|}(\\lambda) \\coloneqq \\sum_{l=0}^{\\infty} (\\gamma \\lambda_{\\verb|GAE|})^l \\delta_{t+l} $\n",
    "\n",
    "where $\\lambda \\in [0, 1]$ is a controlling parameter between bias and variance.\n",
    "\n",
    "As you can easily see, $\\hat{A}_t^{\\verb|GAE|}$ then becomes a diffrence of values without generalization, when $\\lambda=1.0$. (See below equation.)\n",
    "\n",
    "$\\displaystyle \\hat{A}_t^{\\verb|GAE|}(1.0) = \\sum_{l=0}^{k - 1} \\gamma^l \\delta_{t+l} = -V(s_t) + r_t + \\gamma r_{t+1} + \\cdots + \\gamma^{k-1} r_{t+k-1} + \\gamma^k V(s_{t+k}) $\n",
    "\n",
    "**Policy loss**\n",
    "\n",
    "To get policy loss $L_{\\verb|ADV|}$, we'll introduce clipped surrogate loss $L_{\\verb|ADV|}^{\\verb|CLIP|}$ as follows, for more stable convergence. (See [[Schulman, et al., 2017](https://arxiv.org/pdf/1707.06347)].) :\n",
    "\n",
    "$\\displaystyle L_{\\verb|ADV|}^{\\verb|CLIP|} = (-1) \\verb|clip| \\left( \\frac{P(a | \\pi_{\\theta_{new}} (s))}{P(a | \\pi_{\\theta_{old}} (s))}, 1 - \\epsilon, 1 + \\epsilon \\right) A(a) $\n",
    "\n",
    "where $\\epsilon$ is a hyperparameter.\n",
    "\n",
    "In order for convergence, we then take the maximum of clipped objective and unclipped objective for the final policy loss $L_{\\verb|ADV|}^{\\verb|FINAL|}$.\n",
    "\n",
    "$\\displaystyle L_{\\verb|ADV|}^{\\verb|FINAL|} = \\max \\left( L_{\\verb|ADV|}, L_{\\verb|ADV|}^{\\verb|CLIP|} \\right) $\n",
    "\n",
    "(See below picture.)\n",
    "\n",
    "![Clip range](./assets/rlhf_clip.png)\n",
    "\n",
    "**Value loss**\n",
    "\n",
    "In this example, we use $V^{\\verb|ACTUAL|}(s_t) = \\hat{A}_t^{\\verb|GAE|}(1.0) + V_{\\phi_{old}}(s_t)$ instead of $\\sum_{l=0}^{\\infty} \\gamma^l r_{t + l}$ to get the actual values. (Both are equivalent. See above description for GAE.)\n",
    "\n",
    "As [GAE paper](https://arxiv.org/pdf/1506.02438) (Schulman, et. al., 2018) says, we use the following normalized formula as value loss $L_V$ in this example. :\n",
    "\n",
    "$\\displaystyle L_V = \\frac{1}{N} \\sum_{n=1}^N \\frac{\\| V_{\\phi_{new}}(s) - V^{\\verb|ACTUAL|}(s) \\|^2}{2 \\sigma^2}$\n",
    "\n",
    "where $N$ is a training batch size and $\\sigma = \\sqrt{\\frac{1}{N} \\sum_{n=1}^N \\| V_{\\phi_{old}}(s) - V^{\\verb|ACTUAL|}(s) \\|^2}$ (a standard deviation between estimated values and actual values).\n",
    "\n",
    "As this paper says, we also apply clipped objective $L_V^{\\verb|CLIP|}$ to get value loss, such like a policy loss.<br>\n",
    "Now we denote $\\verb|clip| \\left( V_{\\phi_{new}}(s), V_{\\phi_{old}}(s) - \\epsilon \\sigma, V_{\\phi_{old}}(s) + \\epsilon \\sigma \\right)$ as $V_{\\phi_{new}}^{\\verb|CLIP|}(s)$, and define $L_V^{\\verb|CLIP|}$ as :\n",
    "\n",
    "$\\displaystyle L_V^{\\verb|CLIP|} = \\frac{1}{N} \\sum_{n=1}^N \\frac{\\| V_{\\phi_{new}}^{\\verb|CLIP|}(s) - V^{\\verb|ACTUAL|}(s) \\|^2}{2 \\sigma^2}$\n",
    "\n",
    "where $\\epsilon$ is also a hyperparameter.\n",
    "\n",
    "Same as above policy clipping, the final clipped value loss $L_V^{\\verb|FINAL|}$ is then obtained by :\n",
    "\n",
    "$\\displaystyle L_V^{\\verb|FINAL|} = \\max(L_V, L_V^{\\verb|CLIP|})$\n",
    "\n",
    "> Note : In order to prevent from GPU out of memory errors, I have used accumulation training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdaa803-9775-4c9c-b78a-94623c162ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # uncomment when you load RM from the trained checkpoint\n",
    "# rm.load_state_dict(torch.load(\"rm.pt\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1ffabca-5d62-4984-83bb-ca3cac9ac788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get advantages (see above description for GAE)\n",
    "def get_advantage(delta, gamma, gae_lambda, seq_len):\n",
    "    gae_params = torch.tensor([(gamma * gae_lambda)**i for i in range(seq_len)], dtype=torch.float32).to(device) # to float32 (see above)\n",
    "    adv = [torch.sum(delta[:,i:] * gae_params[:(seq_len - i)], dim=-1) for i in range(seq_len)] # list of tensors\n",
    "    adv = torch.stack(adv, dim=1) # shape (batch_size, seq_len)\n",
    "    return adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "278eecca-8a61-46d6-81d2-5715c01e0dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (iter2705) 339/339 - reward 1.2913\n",
      "Epoch 2 (iter2705) 339/339 - reward 2.7226\n",
      "Epoch 3 (iter2705) 339/339 - reward 4.8940\n",
      "Epoch 4 (iter2705) 339/339 - reward 5.0946\n",
      "Epoch 5 (iter2705) 339/339 - reward 5.1048\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "gamma = 1.0\n",
    "gae_lambda = 0.95\n",
    "kl_coeff = 0.05  # beta\n",
    "\n",
    "clip_range_policy = 0.2  # epsilon on clipped policy\n",
    "clip_range_value = 0.2   # epsilon on clipped value\n",
    "\n",
    "num_epochs = 5\n",
    "num_steps = math.ceil(len(dataloader) / gradient_accumulation_steps)\n",
    "\n",
    "# prepare optimizer and scheduler (value model)\n",
    "opt1 = torch.optim.AdamW(\n",
    "    params=value_model.parameters(),\n",
    "    lr=3.0e-5,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    ")\n",
    "sch1 = LambdaLR(opt1, lr_lambda=functools.partial(\n",
    "    _get_cosine_schedule,\n",
    "    num_training_steps=num_epochs*num_steps,\n",
    "    num_warmup_steps=math.ceil(num_epochs*num_steps*0.1)))\n",
    "\n",
    "# prepare optimizer and scheduler (policy model)\n",
    "opt2 = torch.optim.AdamW(\n",
    "    params=policy_model.parameters(),\n",
    "    lr=3.0e-5,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-08,\n",
    ")\n",
    "sch2 = LambdaLR(opt2, lr_lambda=functools.partial(\n",
    "    _get_cosine_schedule,\n",
    "    num_training_steps=num_epochs*num_steps,\n",
    "    num_warmup_steps=math.ceil(num_epochs*num_steps*0.1),\n",
    "    linear_warmup=True))\n",
    "\n",
    "# remove log file if exists\n",
    "log_file = \"reward.log\"\n",
    "if os.path.exists(log_file):\n",
    "    os.remove(log_file)\n",
    "\n",
    "# reward model is always for inference\n",
    "rm.eval()\n",
    "\n",
    "# iterate epoch\n",
    "for epoch in range(num_epochs):\n",
    "    opt1.zero_grad()\n",
    "    opt2.zero_grad()\n",
    "    record_reward = []\n",
    "\n",
    "    # iterate batch\n",
    "    for i, chat in enumerate(dataloader):\n",
    "        itr_batch_size = chat[\"input_ids\"].shape[0]\n",
    "        input_seq_len = chat[\"input_ids\"].shape[1]\n",
    "\n",
    "        #####\n",
    "        # Prepare data with old policy\n",
    "        # - Model is used only for inference.\n",
    "        #####\n",
    "        policy_model.eval()\n",
    "        with torch.no_grad():\n",
    "            # generate tokens with current policy\n",
    "            gen_iids, gen_mask = generate_token_by_policy(\n",
    "                chat,\n",
    "                policy_model,\n",
    "                tokenizer,\n",
    "                max_seq_len,\n",
    "            )\n",
    "\n",
    "            # mask only inference (pred) tokens\n",
    "            seq_len = gen_iids.shape[1]\n",
    "            token_indices = torch.arange(seq_len, dtype=int).to(device)\n",
    "            inf_mask = (gen_mask * (token_indices >= input_seq_len).int()).bool()\n",
    "\n",
    "            # pad left\n",
    "            # (e.g., [[0,1,1,0,0,],[1,1,1,1,0]] --> [[0,0,0,1,1],[0,1,1,1,1]])\n",
    "            last_nonpad_indices = (token_indices * gen_mask).argmax(-1)\n",
    "            for b in range(itr_batch_size):\n",
    "                gen_iids[b,:] = torch.roll(\n",
    "                    gen_iids[b,:],\n",
    "                    shifts=(seq_len - last_nonpad_indices[b] - 1).item()\n",
    "                )\n",
    "                gen_mask[b,:] = torch.roll(\n",
    "                    gen_mask[b,:],\n",
    "                    shifts=(seq_len - last_nonpad_indices[b] - 1).item()\n",
    "                )\n",
    "                inf_mask[b,:] = torch.roll(\n",
    "                    inf_mask[b,:],\n",
    "                    shifts=(seq_len - last_nonpad_indices[b] - 1).item()\n",
    "                )\n",
    "\n",
    "            # trim left\n",
    "            # (e.g., [[0,0,0,1,1],[0,1,1,1,1]] --> [[0,0,1,1],[1,1,1,1]])\n",
    "            first_nonpad_indices = (torch.flip(token_indices, dims=(0,)) * gen_mask).argmax(-1)\n",
    "            start_index = first_nonpad_indices.min()\n",
    "            gen_iids = gen_iids[:,start_index:]\n",
    "            gen_mask = gen_mask[:,start_index:]\n",
    "            inf_mask = inf_mask[:,start_index:]\n",
    "\n",
    "            # the final state is not used for estimation\n",
    "            inf_mask = inf_mask[:,:-1]\n",
    "\n",
    "            # get rewards\n",
    "            rewards = torch.zeros_like(gen_iids[:,:-1], dtype=torch.bfloat16).to(device)\n",
    "            seq_rewards = rm(\n",
    "                input_ids=gen_iids,\n",
    "                attention_mask=gen_mask,\n",
    "            ).detach().squeeze(-1)\n",
    "            rewards[:,-1] = seq_rewards\n",
    "            record_reward.append(seq_rewards.mean().item())\n",
    "\n",
    "            # only the completed sequence is processed\n",
    "            is_eos = (gen_iids[:,-1] == tokenizer.eos_token_id)\n",
    "            is_eos_num = is_eos.int().sum()\n",
    "            if is_eos_num == 0:\n",
    "                continue\n",
    "            elif not(is_eos_num == itr_batch_size):\n",
    "                gen_iids = gen_iids[is_eos]\n",
    "                gen_mask = gen_mask[is_eos]\n",
    "                inf_mask = inf_mask[is_eos]\n",
    "                rewards = rewards[is_eos]\n",
    "\n",
    "        #####\n",
    "        # Run training for value\n",
    "        # - Model is used for training.\n",
    "        # - We use float32 precision (not bfloat or float16) for value computation.\n",
    "        #####\n",
    "\n",
    "        value_model.train()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # get values\n",
    "            values_new = value_model(\n",
    "                input_ids=gen_iids[:,:-1],\n",
    "                attention_mask=gen_mask[:,:-1],\n",
    "            )\n",
    "            values_new = values_new * gen_mask[:,:-1].float() # to float32 (see above)\n",
    "            values_old = values_new.detach()\n",
    "\n",
    "            # get next values\n",
    "            values_next = values_old[:,1:]\n",
    "            values_next = F.pad(input=values_next, pad=(0, 1, 0, 0), mode=\"constant\", value=0.0)\n",
    "\n",
    "            # get delta\n",
    "            delta = rewards + values_next * gamma - values_old\n",
    "\n",
    "            # get actual values r + \\sum \\gamma r (see above)\n",
    "            adv = get_advantage(\n",
    "                delta=delta,\n",
    "                gamma=gamma,\n",
    "                gae_lambda=1.0,\n",
    "                seq_len=delta.shape[1])\n",
    "            values_actual = adv + values_old\n",
    "\n",
    "            # estimate a standard deviation (sigma) of values\n",
    "            values_var = torch.square(values_old - values_actual)\n",
    "            values_var = torch.masked_select(values_var, inf_mask).mean()\n",
    "            values_stddev = torch.sqrt(values_var)\n",
    "\n",
    "            # get value loss (maximum of unclipped and clipped)\n",
    "            values_new_clipped = torch.clamp(\n",
    "                values_new,\n",
    "                values_old - clip_range_value * values_stddev,\n",
    "                values_old + clip_range_value * values_stddev,\n",
    "            )\n",
    "            val_loss1 = torch.square(values_new - values_actual)\n",
    "            val_loss2 = torch.square(values_new_clipped - values_actual)\n",
    "            val_loss = 0.5 * torch.max(val_loss1, val_loss2) / values_var\n",
    "            val_loss = torch.masked_select(val_loss, inf_mask).mean()\n",
    "\n",
    "            # optimize value model (critic)\n",
    "            val_loss.backward()\n",
    "            if ((i + 1) % gradient_accumulation_steps == 0) or \\\n",
    "               (i + 1 == len(dataloader)):\n",
    "                opt1.step()\n",
    "                sch1.step()\n",
    "                opt1.zero_grad()\n",
    "\n",
    "        #####\n",
    "        # Run training for policy\n",
    "        # - Model is used for training.\n",
    "        #####\n",
    "\n",
    "        policy_model.train()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # get logits\n",
    "            logits_new = policy_model(\n",
    "                input_ids=gen_iids[:,:-1],\n",
    "                attention_mask=gen_mask[:,:-1],\n",
    "            )\n",
    "            logits_new = logits_new.logits\n",
    "            logits_old = logits_new.detach()\n",
    "\n",
    "            # get propability P\n",
    "            logprb_old = -F.cross_entropy(logits_old.transpose(1,2), gen_iids[:,1:], reduction=\"none\") # get log probability (see above description)\n",
    "            logprb_new = -F.cross_entropy(logits_new.transpose(1,2), gen_iids[:,1:], reduction=\"none\") # get log probability (see above description)\n",
    "\n",
    "            # get advantage loss with clipped objective\n",
    "            prb_ratio = torch.exp(logprb_new - logprb_old) # P_new / P_old\n",
    "            prb_ratio_clipped = torch.clamp(\n",
    "                prb_ratio,\n",
    "                1.0 - clip_range_policy,\n",
    "                1.0 + clip_range_policy,\n",
    "            )\n",
    "            adv = get_advantage(\n",
    "                delta=delta,\n",
    "                gamma=gamma,\n",
    "                gae_lambda=gae_lambda,  # 0.95\n",
    "                seq_len=delta.shape[1])\n",
    "            pg_loss1 = -adv * prb_ratio\n",
    "            pg_loss2 = -adv * prb_ratio_clipped\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2)\n",
    "            pg_loss = torch.masked_select(pg_loss, inf_mask).mean()\n",
    "\n",
    "            # get KL loss\n",
    "            # (see https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/04-ppo.ipynb)\n",
    "            l_old = logits_old - torch.amax(logits_old, dim=2, keepdim=True) # reduce quantity to avoid overflow\n",
    "            l_new = logits_new - torch.amax(logits_new, dim=2, keepdim=True) # reduce quantity to avoid overflow\n",
    "            e_old = torch.exp(l_old)\n",
    "            e_new = torch.exp(l_new)\n",
    "            e_sum_old = torch.sum(e_old, dim=2, keepdim=True)\n",
    "            e_sum_new = torch.sum(e_new, dim=2, keepdim=True)\n",
    "            p_old = e_old / e_sum_old\n",
    "            kl_loss = torch.sum(\n",
    "                p_old * (l_old - l_new + torch.log(e_sum_new) - torch.log(e_sum_old)),\n",
    "                dim=2)\n",
    "            kl_loss = torch.masked_select(kl_loss, inf_mask).mean()\n",
    "\n",
    "            # get policy loss\n",
    "            total_policy_loss = pg_loss + kl_loss * kl_coeff\n",
    "\n",
    "            # optimize policy model (actor)\n",
    "            total_policy_loss.backward()\n",
    "            if ((i + 1) % gradient_accumulation_steps == 0) or \\\n",
    "               (i + 1 == len(dataloader)):\n",
    "                opt2.step()\n",
    "                sch2.step()\n",
    "                opt2.zero_grad()\n",
    "    \n",
    "        # print log\n",
    "        print(f\"Epoch {epoch+1} (iter{i+1}) {math.ceil((i + 1) / gradient_accumulation_steps)}/{num_steps} - reward {seq_rewards.mean().item() :5.4f}\", end=\"\\r\")\n",
    "\n",
    "    # save logging\n",
    "    epoch_average_reward = sum(record_reward)/len(record_reward)\n",
    "    print(f\"Epoch {epoch+1} (iter{i+1}) {math.ceil((i + 1) / gradient_accumulation_steps)}/{num_steps} - reward {epoch_average_reward :5.4f}\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        for r in record_reward:\n",
    "            f.write(\"%s\\n\" %r)\n",
    "\n",
    "# save checkpoint\n",
    "torch.save(value_model.state_dict(), \"value.pt\")\n",
    "policy_model.save_pretrained(\"./llm_aligned\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32124ea5-cfd3-4ab6-afe6-396e32eaca20",
   "metadata": {},
   "source": [
    "Here I show the reward transition during PPO training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52d8fff2-b114-418f-b61c-c8a9d9008b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAGdCAYAAAAmK7htAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQ2lJREFUeJzt3XdcU+f+B/BPWGGDgoAouBUnbovbuuXa9ra3Vmuttb1t9WrVDluptdreKnT8unutnXZZq612uerAWSeKgnuLA3GxIYw8vz+QQw5JgECSk/F5v16+Xmc8Sb45AvnkOc9zjkoIIUBERERkA1yULoCIiIioHIMJERER2QwGEyIiIrIZDCZERERkMxhMiIiIyGYwmBAREZHNYDAhIiIim8FgQkRERDbDzdovqNVqceXKFfj5+UGlUln75YmIiKgWhBDIyclBeHg4XFws169h9WBy5coVREREWPtliYiIyAzS0tLQuHFjiz2/1YOJn58fgLI35u/vb+2XJyIiolrIzs5GRESE9DluKVYPJuWnb/z9/RlMiIiI7Iylh2Fw8CsRERHZDAYTIiIishkMJkRERGQzGEyIiIjIZjCYEBERkc1gMCEiIiKbwWBCRERENoPBhIiIiGwGgwkRERHZDAYTIiIishkMJkRERGQzGEyIiIjIZjCYEBGRXVq+Lw07T99QugwyMwYTIiI7kHThNqb/eBBFJVqD+z9JPI2ms1fji+1nrVyZMv4+fQMv/nIY47/Yg9t5RUqXo5jiUsM/D/aMwYSIyMY9+1MyHlj0N34/dAWtX1mrt3/f+Vt4e/0JAMAbq49hxf40pF7OsnaZJlmbchXbT12v9eM3HsuQluNWppijJLszZ1UKWs1Zi+92X1C6FLNiMCEismHD3tuKVQcvV9lm47FrsvVZPx/GPz7agRu5GjSdvRpNZ6/GLSv1Kggh8OvByziUlon952+hxMA3+vm/H8GUHw5gwpd7kZFdaPJrXLqdj692npPW1x1Jr1PNuoQQGPB2IprOXg2tVpjteSs7dyMPmpJSo/uLSrSIW5mCPw9fMdrmhz0XAQBzf01Fx/nrkVVQjKazV6NPwmaz12tNDCZERAA0JaX4ad9FXM0qULoUSdKF2zh5LVdv+81cjWx98VbDp2+e/i5JWn7x58NmrW1dajr6vbUZ1yoFi11nb2LmT8m495Od+Nenu9Byzlp88/d5WZslOuv3fbLT5NNPz/x4sLZlV+u9jadw4WY+AOD9jSct8hpbTmRg0Dtb0CchUW9feRj6dtd5/Lj3IqYtPYjtp64j8USGXltdOYUliH7tLwDA5Uzb+RmuDQYTInJ6x65m4+53tuKlX1Iw7rPdSpcjeWDR3wa3f7urout+77lbRh+fdOG2tOzuqqpVDZczC/D0d/uRkSMPIJO/T0LarQL0WrgJG4+W9di8sOIQHv58j95zzPv9iNHnv5JViDdWH0NWfnGNazp4MVO2HuzrUePHVufDTacqljefrrZ9flEJsgvLeip6LdyIXE1JtY957Ot9AIAbuRpZj1LT2avR/OU1+HzbWbyx+pi0fcKXezHp633IKqg4RttOVn0azNhYJHvAYEJETm/kB9ulb5nn73xbro3iUi3iVh7Gb8lVn3qpqw82nULT2avx3a7zGLN4l7S9f+sGRh+zNjUdS+90/RtT+bSLEAJ9EjZj/ZFr6Llgk7R99eGrsnb//nY/buUV4eekS0af+8adXp7CYsOnL6Jf/wsXbuZVWR8Ag6dXbuYVKfJBnKspQZ+Ezeg0v6yn4lq2Bh3mrUd+UfXhpFzLOWvRaf562ftasOaYwba6oemPQ8ZP8ZTXZq8YTIjIqSSeyEDT2aul8FBq4INOiNqNLUg8noEf96ZhxrLkupRYY3N/k/dEfP5otyrbv7zK+CDR9UfS0XLOWlm4+NzIKZapSw/obXvnrxNVvval22XB7/kVh4y2GfD2lmpnmcxeqX9KSgjgRHqO0ceUj7NZojMuRf85hKxHotzk75JQUGQ4TB1Ky8RtAz0935s4GDW7sATNX15Tbbsvd1TUf+Ka8fcL6IdMe8JgQkROZdKdbvTy8LD5uP65+5xafNvUlJTiKZ0xHcY+zG7lFSHlUtUzZm7kapCnU8OvU/vgfEIs/je+q9HHjI4Oh9rNFSPah1X53FeMjD8oH4/ygk5wWLjmuF67nELDp1wq98a8cV8HfDahIih9vPk0zt3I0+ttqeypb/dXuX/5/orgdGjeMGl59Mc79NpuP3VdOs0EAPP/OIqms1djbUpFDeUDUJvFrZHGaOhadyQdbV9dh9+SL+Oxr/fi8KVMaV+JkcGxC9cct+jA2VKtwGGdn6G/nu0vLXdqHIBDrw5DiL+nxV7f0tyULoCISCljFu8yOEbjcFoW+rYKNum53ttwSrbe9tV1OJ8QK9t2OiMXQ97dCgDwcnfFz1Ni0D48QNbmsa/3YssJ+fiBhgFlHzJVhY4Px3YGAHw6oRvO38jDwHe2AAD2vjwYPRdWnIbpnbAZu+MGIyyg4oOr8tTizPwivP7HUb3X+PXgZbjpjFW5r3M4fk02fErh4Z6RcHGpaLvx2DXsv2B8PEy5xBP6Yyfyi0pw7kYe2jX0l20P8HKXrZ+6loNWoX4AynoMJny51+BrTPlB3uMz4a4m1dZVHmS3nLgu/b9eyzI+o2jBmmN4JbYtTmfkIjLIG2o3VwBV9+zURFZ+Ma5UGqDdOtQPB+YORYCXO1xdajeWyJawx4SInEblUzTGBo5O+1H/VIUurVZgxf40nL1eMWPmlIGudd3Xu5GrkUIJABQUlyL2w4pv+ZqSUmQVFOuFEgAIvfPt18VFhaX/7qW3/1z8KKhUFR9ITYN9cPKNkTi7cBSCfdV67fu/lYi4lYel+v7xkby3ofPrG7DSwBTlmT8lY9rSihkx74/totcGAN56oJMslJTL1DntEduxIQ7OHYoXhrXGsqfukrVbdfAStFqB6Nf+QtPZq9Hu1fWI/XAH5usMon0lti0AYGSHirC2++xNaTnfyFgWQ2p7HZBLVcx++XLHOaxLTcfQ97Zhsk5P2rM/JVf7vIkvDAQAzBnVFucTYrH9xUHSvmHvb4WLzv91+bGr7+PhEKEEYI8JEZlRyqUs/JZ8GTOHtoav2vb+vGRWMfMjpnkQdt35YAus9E1cV3pWIe6Kr+iBKP/2vMnAKaFmcWXjBmaPjMLvRnoWyrV5ZV2V+8v1bhmM8wmxyC8qwU/70jA4KlQWSsp5uFV874zt2BCrdU5fFJVq8ePeNAxo3QDDqzn1Y6oVk2PQo2l9af2XKb0Nzi56uFck6vl4YNrdrQAASyb1kGarPPvTIaw8cFlvzMc3OrORJvVpBgCIqO8tbbueUzGN+lBaZq3fQ/cm9fDs0NYY/4X+DCMAaPnyGr3TOB5uLugQ7o8DOjOGyntmEk9cxzM/HsS7Y6KRpzMw9tCrwxDg7Y5ley9i9p2LxH3zeE80C/aR9bbpvsdr2RqsPFhxOuuu5kG1fp+2ij0mRGQ2oz/egS92nEOHeettcvDdN7vOG913T+dwtA71BVA2M8dQDwgAWSgBynpFDA2g1ZWw9jiOXs02ur+qx3/1WHeD27093DCpTzNEBnkb3K/ro3FdsHfOYL3tk78/gLfWVz1oFQC23PkGXxPdm9STrXertF6udwv5B2q7cPlpmu2nqr4HTnnvwPTBraRtutN7q5pGXZWnBzTH8qdj0KdlMDbojN3QVTmUTBvUEiffGIkHu0cYfd4/Dl3Bj3svStdIqe/jgQDvsgA8tmckzifE4lz8KAwwMrPqvs7hAIBBbRrAw9WxP7od+90RkdXoDjIEgLvibe/qk8Z6LT59pBvG9ojAwDYh0rb5fxi/9oauZnFr0EJnRkVPnd6CmhCi7LSQMf6exntvasrFRYUQP09MG9RSb9+iLWek5fIPP11bXhiIpsE+etu/e6InAODw/IoBqIsndDPYe1PZxJgmeu0aGDjlVBOVe+bKr+/xkU5Iie3UEOfiR+F8Qiz2vzKkyueLG9lWOg3VKtQPZxaOqraG8tA5pnsEfjBwqq3cqzqzqAyNa6nq2A2KKvvZ1JRopTFBhv6/HAGDCRHVWX5RCf5daTbFjVyNrGu9JgqLS02+amVmfhHWpV6VXceiqESLzcevoens1ZjyfRJ+SbqEprNX4+yNsutktAzxlT3HiA5hUKlUGKjzbfX8jbJvtlcyC0zq/RnbMwLbZg0yuv//HozGsddHSOuXbhfAy8NV1mZI24qA1KieV41fuzpNquldeeOfHXFg7lA82K0x3nqgE84nxBoMJQAQHREIoCw4nU+Ixck3RtbotFCLBj547d4OettVKhUeuSuy+jcB4M9n+srWd8dV9AY9+tVeNJ29Wrb/k4e7Sh/6wb5qHJo3DL2a1cfgqBDoZoF1M/vpvZariwqP3zltZMyzQ1pLbfu0rNmg6Sf7N69Ru3LlA33/PnNTGiD8ezXXMrFXJp0Enj9/Pl577TXZtjZt2uD4cf0pZUTk+IQQOHIlW2/wZLkeCzbqzUwxplQrMOL9bTh/Mx+LJ3TD8PZhEELg7I08RNb3xuKtZ7AmJR2/TesDd52u7Jj4zSgoLkWAlzvefKAjsgtLZJdfX5uajrWp8nupjOneGNGNA/HwF3sw/e6KUwHddXo7LmcWyD7gdscNRn2f6q8w+s8ujaBSqXA+IRZ/HUmXTSEGgKHtQ2VBJH7tMbi5VLyf8oGs+87fgosKaBhgvmByf9fG+CTxtMGLyJ1dOKqsp0ANvP1gtN7+Nx/oiJd+SYGXuytWTI7R68nRHdNS2erpfaWBvn9UChW63rivI0Z1aIiHK43tSH1tOFIuZWHc52VX5e3QSD6TSXeGUWWv/qOd3rYAL3f89HQMAOB0Rg6W7U3DlIEtEGSk1+bV0e3w6uh2eGf9CXycKL8a7NIne6Fj4wCDj6uKT6UwWp0gH/3anuxnWrixFyaPTmvfvj02btxY8QRutjfAjYisY+A7W6Rz5tXZevI61G4uBgfrdZq/HtmFFYMCn/4uCecTYpGw9jgWb5Nf5KvVnLK7684Z1RZP9m+OgjuzL7IKijH5+6pn05RrFOiNXs2DkDJ/GLw9Kv6GVfXhWnlsiSG74u6WdccPax+Gv57tj2HvbZO2+VU69bAmRR6ayh/fw8RTQjXh6qLCllmDkFVQrHfNDkOzaHQ91CMSD/WoWY9GZe3DA2ocUHtX6nGY+4928FW7IaZFEOaNbifNUKpsxuBW+GDTKb3tj/eturejZYgfXjEQXgwp7yUCgBdHtMGD3SLQwE8/MESF+eH4nWnBo6PD9a7SeuS14TU65aXLUPi6v2tjk57DXph8KsfNzQ1hYWHSv+Bg0+b6E5HjMBRKTrwxQraeXViM6zkaTPxqL8Z+ttvglT11Q0m5prNX64USXQvWHMMvVVwCvSq9mpd96OuGknIvjYiq0XP8MU3+zT/1teEGezdah/oh9bXhmD0yCttmDTL5A8kSArzccWrBSGn9gzvXQLEVT905zdGhkT+e0AkWk/o0w6iODQ0+pn2lwbOWMKhNxam+/wxsaTCUAMCiRyouLPfh2M6YM6ot3v5XJwBlp7K8TewtAfTvB+Tt4YoWDQyfZrN3KmHCtZfnz5+Pt99+GwEBAfD09ERMTAzi4+MRGWk8RWs0Gmg0FeeZs7OzERERgaysLPj7W/4HiYgsp/K5/PJvxWtTrkpTJT1cXfDhuM5Sb8aOlwahcb2KsQ4lpVq0vNMLYg3fPt6zynvKaLWiRpcHP58Qi8LiUny76zwGtQmRLuxVU2+vP45PEs/Iti1/OgY9m5m/p6QqxaVa2akxW1FSqoWbCXUlXbitNy353THRivUqaLUCKpV8QGthcSk8XF2q7Z0y5sLNPDz57X588nBXk3/ezCE7OxsBAQEW//w26aexV69eWLJkCdatW4dFixbh3Llz6NevH3JyjF/JLj4+HgEBAdK/iAjj06mIyH7o3rm2siidK3QWlWplp1gqD4i1VCjpEhmot+3swlFVhhKg+lMaujzdXfFU/xa1+pDQneZaztqhBIBNhhIAJoUSQL9H4ezCUYqe6nBxUen1jnm6u9Y6lABAkyAf/PXsAEVCiTWZ9D8/cuRIPPjgg+jUqROGDx+ONWvWIDMzE8uXLzf6mLi4OGRlZUn/0tKMT4sjIvtR+dvp2B4VXzqaGZnJAZRd1vyPQ1dw5nou0m7JTwV5uLnI7vtRmZ9nzce0rfpPH0RXGpRYlw8Fc1O7uWJMd8ccI6AE3dMqD3WPsKn/azJNnUauBgYGonXr1jh9+rTRNmq1Gmp17eanE5FtOlnp4mOJLwxEo0AvvW2D7tyvRdc3uy7IruCpa+usgXrjNOb+ox3+++dR9GsVjCWTesquGVJu/ytDsCblKsb2iMStvCJ4q8vO4X/zeE90fn0DAOBf3WoeAj59pCt2n72Foe1CsXDNMRy5ko0AL3fpSqSGLvNeG/NGt5duSueo4wWsxdvDDV8/1gNnrufi3w46W8VZ1CmY5Obm4syZM5gwYYK56iEiG1R+/l7t5oI/nukrm2XiojLcQ1JVr4khhmZtbJ01EE2CfGQDIA/NG4ZLt/Ol6afdm9RDsK8aj8Y0BSCfvRDo7YEjrw3H9lM3jF5R05ARHRpiRIeyQZY/T+6No1ez0SUiEFezC/HVjnOYMrCFSe/NGN1BkJ8/avgKr1Rzg6JCpAuRkf0yafDrCy+8gNGjR6NJkya4cuUK5s2bh+TkZBw9ehQNGtTsl95ag2eIyHTrUtPxw54L+GhcFwR6l52z15SUVnkfl52z79brLSlXXKpFvzcToVKVfQifuZ5n9HlqOp203JnruUg8noFxPSPhY4P35TGFEMImZusQVcVan98m/TZfunQJ48aNw82bN9GgQQP07dsXu3fvrnEoISLbNvn7souBdX59A47/dwQeX7IPf5+5WeVjjIUSoGxg5e6Xy67KWaoVBk/DAGUXqTJViwa+aNHAt/qGdoChhKiCScFk2bJllqqDiGzM4P/bWu3l4T99pGuNn8/YLdn/78Fo9G7B6yERURnbnCdGRFanKSmVrdfknjWmns9PfGEg+rUKxk9P3YVWd+5XwzEBRKTLvk/MElGdCSHw7a4L+Hy78ausGtIlMhBqN9OuYNks2AffPVF22uavZ/tDU6KFp7vpV8EkIsfFHhMiJ7fz9E3M+/0ILt2uuodE91Lz/+rWGCvu3ASttlQqFUMJEelhjwmRkxJC4Ivt57BgzbFq2z7dvznUbq7YHTcY205dx72dw02+MicRUU0wmBA5qb/P3DQYSl4eFYWFa44DAL5/ohf6tqoYmBoW4Ikx3XlbCSKyHAYTIidU+eZ7up7s1xyP3NUEJVoBf093K1ZFRMRgQkQ65oxqC5VKBW8P/mkgImXwrw+RE7mZq0FRqdbgvj+f6YsOjQIM7iMishYGEyIH9Nm2M9I4kbE9IrBsXxqaB/vg7A39S8IH+Xjg1dHtGEqIyCYwmBA5mMrjR5btSwMAg6Fk/cz+aBPmZ5W6iIhqgvP9iJwYQwkR2RoGEyIHkl9UonQJRER1wmBC5EBe+iWlxm3feqCTBSshIqodjjEhcgBXMgvQO2GzbNszd7fER5tP67Ud0T4Mn07oZq3SiIhMwmBCZMfe3XASpzNysOHoNb19zw1tLQWT8pk5Sa8MQX0fD2uXSURUYwwmRHZKCIEPN50yuC/Y1wMqlQrnE2KlbQk8dUNEdoBjTIjsVMf5fxndtytusBUrISIyHwYTIjuVqzE+A8edd/4lIjvFv15EDuanp+5SugQiolrjGBMiBxHip8beOUOULoOIqE7YY0JkpxoGeMrWM3I0ClVCRGQ+DCZEdmjbyeu4mlUo29Y82EehaoiIzIfBhMgOPfrVXmk5/E7PybdP9FSqHCIis+EYEyIbJoSASqWSbdOUlMrWN78wEHmaEgT5qq1ZGhGRRbDHhMhGXc4sQM+Fm/D+xpOy7W1eWSdb93R3ZSghIofBYEJko55fnozrORq8v9Hw1V0BoHE9LytWRERkeQwmRDZq99lb0vKOUzekZT91xRnYtTP6WbUmIiJLYzAhsjEXb+ZjxrKDsm2PfLkHQggAQKtQXwDA4gnd4OfpbvX6iIgsiYNfiWxM/7cTDW6/nV+M+j4e0qXofdX89SUix8MeEyIbUlhcanTf7fwiAEBOYVkw8WdvCRE5IAYTIhuiKdEa3Xc7rwilWiFdWM3fiz0mROR4GEyIbMjcX1P1tnWOCARQdion9sPt0vZgThEmIgfEr1xENiKnsBi/H7qit72+jwcA4Mlv98u2+3CMCRE5IPaYENmAM9dz0XH+X7Jt7z0Ujc3PD4C3h6tee7Ubf3WJyDHxKxeRDZi29KDetn92aQwAOHDhtt6+Aa0bWLwmIiIl8GsXkQ04djVbtv7umGhp2dNAj8k9ncMtXhMRkRIYTIhsTKNAL9zftbG07uOh37H5j04MJkTkmBhMiGzM5cwC2XrcqCjZ+rqZvAw9ETkuBhMiG9e7RbA0puTswlGICvNXuCIiIsvh4FciG+Dn6SZd0dWQbx7vacVqiIiUwx4TIoWVlGploWT19L4KVkNEpCz2mBApLFsnlJxeMBJurvy+QETOi38BiRSWeefmfH6ebgwlROT0+FeQSGEXbuUDQJVjTIiInAWDCZHCXlh+SOkSiIhsBseYECnkdl4RfjlwCTfzipQuhYjIZjCYEClk1s+HsfHYNWnd0M36iIicDU/lEClEN5QAwPzR7RWqhIjIdjCYENkIL/aYEBHxVA6RtQkh8K9Pd+ltv8WxJkRE7DEhsrbUy9lIunBbb/uEu5ooUA0RkW1hMCGyspzCYoPbXVxUVq6EiMj2MJgQWdmrvx/R2/bhuC4KVEJEZHs4xoTIyk5n5MrWk18dikBvD4WqISKyLewxIbKi4lKtbP2XKb0ZSoiIdDCYEFnRd7suSMtuLip0a1JPwWqIiGwPgwmRFX227ay0/Of0vgpWQkRkmxhMiKwoPbtQWm5S30fBSoiIbBODCZFC1G789SMiqox/GYkUwuuWEBHpYzAhshIhhLT8xaPdFayEiMh2MZgQWUlBcam0HNMiSMFKiIhsF4MJkZXkFpYAAFxUgDfvJExEZBCDCZGV3Lxz92CtAFQqji8hIjKEwYTIwoQQ+P3QFYz8YLvSpRAR2TwGEyIL237qBqb/eFDpMoiI7AKDCZGFJaw9rnQJRER2g8GEyMKOXs2Wrft78qbeRETG1CmYJCQkQKVSYebMmWYqh8jx7Z0zROkSiIhsVq2Dyb59+7B48WJ06tTJnPUQObTwAE94unOqMBGRMbUKJrm5uRg/fjw+//xz1KvH27YTGZN6OUu2vnP23QpVQkRkH2oVTKZOnYrY2FgMGVJ9l7RGo0F2drbsH5EzKC7V4h8f7ZDWd86+m9cvISKqhsmj8JYtW4YDBw5g3759NWofHx+P1157zeTCiOzdkp3nZevhAZ7KFEJEZEdM6jFJS0vDjBkz8MMPP8DTs2Z/ZOPi4pCVlSX9S0tLq1WhRPbm8+1nZevsLSEiqp5JPSZJSUnIyMhA165dpW2lpaXYtm0bPv74Y2g0Gri6ygf2qdVqqNVq81RLZEeaBvsgI0ejdBlERHbFpGAyePBgpKSkyLZNmjQJUVFReOmll/RCCZEzy7lz0z4A6N+6gYKVEBHZD5OCiZ+fHzp06CDb5uPjg6CgIL3tRM7umM6F1aYMaKFgJURE9oNXfiWygl7N6itdAhGRXajztbG3bNlihjKIHMu17EJpuU/LILi4cOArEVFNsMeEyAJWHbwsLSfcz6sjExHVFIMJkQWcupYrLUfU91awEiIi+8JgQmQBvxy4pHQJRER2icGEyAIejWkCAPBV13kYFxGRU2EwIbIAtVvZr9bDvSIVroSIyL4wmBBZQK6m7OJq7DEhIjINgwmRBeRqSgEAPgwmREQmYTAhsoC8Oz0mfgwmREQmYTAhsoDcO/fJYY8JEZFpGEyILEAaY+LJYEJEZAoGEyILqBj8yjtuExGZgsGEyALypGDirnAlRET2hcGEyAJu5hUBAHzYY0JEZBIGEyIz23XmprTsxx4TIiKTMJgQmZEQAuM+3y2ts8eEiMg0DCZEZnTuRp5s3c2Vv2JERKbgX00iMzqRniMt1/PmaRwiIlMxmBCZ0YGLt6Xluf9op2AlRET2icGEyIxaNPCVlu+JDlewEiIi+8RgQmRGSRfKekw6RwRyfAkRUS3wLyeRGa1IugQASE7LVLYQIiI7xWBCRERENoPBhMhMsguLpeW3/tVJwUqIiOwXgwmRGQgh0Gn+X9J62q18BashIrJfDCZEZpBdUCJb/8/AlgpVQkRk3xhMiMxg/4VbsnUvD16KnoioNhhMiMzgiW/2K10CEZFDYDAhqqMtJzKULoGIyGEwmBDV0fPLD8nWd8cNVqgSIiL7x2BCVEc384qk5S8e7Y6wAE8FqyEism8MJkRmNKRdqNIlEBHZNQYTIiIishkMJkRm4sMpwkREdcZgQlQHVzILpGUvDzcFKyEicgwMJkR10Dths7T8QNdGClZCROQYGEyIzOTZoa2VLoGIyO4xmBDVUp5Gfn8cT3eOMSEiqisGE6JaWrTljLQczmuXEBGZBYMJUS19nHhaWl75nz4KVkJE5DgYTIjMgFd7JSIyDwYTIiIishkMJkR15OnOXyMiInPhX1SiOlo9vZ/SJRAROQwGE6JaKC7VSstBPh4KVkJE5FgYTIhqIb+oVFr24j1yiIjMhsGEqBYK7gQTNxcVPFz5a0REZC78i0pUCyeu5QAASrQCKpVK4WqIiBwHgwlRLUz8aq/SJRAROSQGEyIiIrIZDCZERERkMxhMiEx0M1cjLQ+OClGwEiIix8NgQmSirIJiaXn+Pe0VrISIyPEwmBCZKE9TNlVYpQIi6nsrXA0RkWNhMCEy0YGLtwEAQihcCBGRA2IwITLRvN+PKF0CEZHDYjAhIiIim8FgQmSi1qG+ADgjh4jIEhhMiEwU6u8JABjVsaHClRAROR4GEyITZReWAAACvNwVroSIyPEwmBCZKOfOdUz8GUyIiMyOwYTIRNmF5cHETeFKiIgcD4MJkQmEEMguKDuV4+/JHhMiInNjMCEygaZEi6JSLQDAz5M9JkRE5sZgQmSC8vvkuLqo4KtmMCEiMjcGEyITXM8pu7Owl7srVCqVwtUQETkeBhMiE/zjox0AgFxNicKVEBE5JgYTIiIishkMJkRERGQzTAomixYtQqdOneDv7w9/f3/ExMRg7dq1lqqNyGb1b91A6RKIiBySScGkcePGSEhIQFJSEvbv34+7774b9957L44c4W3gyfFdzSqQlt1dOPCViMgSTJrvOHr0aNn6ggULsGjRIuzevRvt27c3a2FEtuZWXpG0PH1wKwUrISJyXLW+EENpaSlWrFiBvLw8xMTEmLMmIpuUX1QqLUdHBCpXCBGRAzM5mKSkpCAmJgaFhYXw9fXFqlWr0K5dO6PtNRoNNBqNtJ6dnV27SokUlndninD7cH+FKyEiclwmz8pp06YNkpOTsWfPHkyZMgUTJ07E0aNHjbaPj49HQECA9C8iIqJOBRMppbzHxMeDV3wlIrIUk4OJh4cHWrZsiW7duiE+Ph7R0dH44IMPjLaPi4tDVlaW9C8tLa1OBRMppbzHxFvtqnAlRESOq85f/bRarexUTWVqtRpqtbquL0OkuB2nbwAArmUb/3knIqK6MSmYxMXFYeTIkYiMjEROTg6WLl2KLVu2YP369Zaqj8gmaLUCvyVfAQAcu8pxUkRElmJSMMnIyMCjjz6Kq1evIiAgAJ06dcL69esxdOhQS9VHZBPWpqYrXQIRkVMwKZh8+eWXlqqDyKYduZIlLT9zd0sFKyEicmy8Vw5RDRQUV1zD5NjVHAUrISJybAwmRNVIuZSFr3eel9anDmqhXDFERA6OwYSoGl/tPCctD2jdAF0i6ylYDRGRY2MwIaqGSud+ff1aBStXCBGRE2AwIarGygOXpeXrObyGCRGRJTGYEJkgVWd2DhERmR9v+kFkhFYrsPXkddm2GYNbK1QNEZFzYI8JkRE/7L2ISUv2ybb1bFZfoWqIiJwDgwmREZ9uOSNb/2hcF4UqISJyHgwmREZcziyQrcd2bKhQJUREzoPBhMiIZsE+0rKfpxtcXFRVtCYiInNgMCEyIDktE+du5Enry5+OUbAaIiLnwWBCZMB9n+yUlt8dE422Df0VrIaIyHkwmBBVIoSQre88fVOhSoiInA+DCRGAXE2JFEg0JVrZvskDmitREhGRU2IwIaf39+kb6DBvPeLXHseRK1mImrtOtr9JkI+RRxIRkbnxyq/k9B7+Yg8A4LNtZ/HZtrOyffH3d4SHG/M7EZG18C8ukRH1vN0xrmek0mUQETkVBhNyakequCnf7fxiK1ZCREQAgwk5MSEEYj/coXQZRESkg8GEnNaus5wGTERkaxhMyGkdvmT8NA4A/PlMXytVQkRE5RhMyGl5uFb8+PdqVl+2b830fujQKMDaJREROT0GE3Jar/95FADQNMgbnz7STbavXTgvQU9EpAQGE3JKvyVflpbP38xHPR8PHJo3DE2CvPHB2M7KFUZE5OR4gTVySm+uPS4tv/WvTgCAAC93bJ01SKmSiIgI7DEhJ7Tx6DVcySqU1ge1CVGwGiIi0sUeE3IK6VmFuCt+E6IbB+BQpdk4/l78NSAishXsMSGnMOHLsvvhVA4lgd7uULu5KlESEREZwGBCTiHAy93g9rce6GTlSoiIqCoMJuQU9l+4bXB7YYnWypUQEVFVGEzIqdXzNtyTQkREymAwIacV26kh+rYMVroMIiLSwWBCDufHvRfx7oaT0nqupsRgu08e7gqVSmWtsoiIqAY4T5IcSn5RCeJWpgAAohsHYHDbUHy65Yxeuzcf6Gjt0oiIqAbYY0IOJTO/WFp++rskAMDJaznStmfubolezerj3s6NrF4bERFVjz0m5FBu5RVJyyVagcLiUnh7VFyn5PlhbZQoi4iIaojBhBzKVzvPydaj5q6TljmchIjI9vFUDjmUlQcuG90nhBULISKiWmEwIacRFeandAlERFQNBhNyGKXaqrtEpg5qaaVKiIiothhMyGHczi+qcv+ojg2tVAkREdUWB7+Sw7h0u8Dg9vcf6ox7O4fzYmpERHaAwYQcxjd/nze4/b4uvGYJEZG94KkcchirDlbMyJk1vOx6JeEBnkqVQ0REtcAeE3IIlQe+ThnQAjEtgtA2zF+hioiIqDYYTMghXMsulK27uKjQNbKeQtUQEVFt8VQOOYTeCZul5af6N1ewEiIiqgsGE7J7yWmZsvWXRkQpUwgREdUZgwnZvfs+2Skttw/3h6sLpwUTEdkrBhNyKON7NVG6BCIiqgMGE3Iobq7sLSEismcMJmS3bucV4d/f7JNt6xIRqEwxRERkFpwuTHYp7VY++r2VKNv2wdjOaBXKOwgTEdkz9piQXaocSgBgePswBSohIiJzYjAhuyOEMLhd7cYfZyIie8e/5GR3TmXkGtzOuwcTEdk/BhOyKxdv5mPYe9v0ts8c0kqBaoiIyNwYTEgRBUWluHAzz+TH7Th9Q7Ye7OuBfq2CMXNIa3OVRkRECuKsHFLEmMW7kHI5CwBwduEouNTwaq0Xb+XL1ve+PAQ8g0NE5DjYY0JWl3YrXwolAHCw0r1uqrI29aps3cVFxbElREQOhMGErK7yVF8P15r/GF64WdFjUt/Hw2w1ERGRbWAwIcX9cuBSjdpl5RdLyy4qYNuLgyxVEhERKYTBhBS35O/zBrevTbmKprNXo+ns1Th7PRfRr/8l7dv43AD4qjlEiojI0TCYkE3acPQapvxwQFq/+/+2yvZH1ve2dklERGQFDCZkczQlpXjy2/1VtnEzYVwKERHZD/51J6vSvZz8fZ3DpeWSUq20fDuvGERE5JwYTMiqEtYel5bn39NeWj5xLUdavit+U5XP0SSIp3GIiBwVgwlZzfUcDRZvOyutB3pXTPd9ZulBAGWncaoya3gbrHg6xjIFEhGR4kwKJvHx8ejRowf8/PwQEhKC++67DydOnLBUbeRg1h1JN7rv7I2yy9O/9sdRo22GtA3F1EEtEeLvafbaiIjINpgUTLZu3YqpU6di9+7d2LBhA4qLizFs2DDk5Zl+zxNyPnN/Ta1y/8aj17B0z0XZtmmDWkrLHz/cxSJ1ERGR7TDpQhDr1q2TrS9ZsgQhISFISkpC//79zVoYObbfpvbR2/bvSjNxzsWPgkqlwlMDmkMIwNPd1VrlERGRQuo0xiQrq+x+J/Xr1zfaRqPRIDs7W/aPnM/aFPk9bqIjAgEAJ98YafQx5ffA8fd0R4CXu8VqIyIi21HrS2dqtVrMnDkTffr0QYcOHYy2i4+Px2uvvVbblyE7t2zvRcxemSLbNqJ9mLTs4WY4G4f6qy1aFxER2aZa95hMnToVqampWLZsWZXt4uLikJWVJf1LS0ur7UuSHaocSgAgqqFflY95aUQUdscNtlRJRERkw2rVYzJt2jT8+eef2LZtGxo3blxlW7VaDbWa336pwtP9W8jWQ/zUyMjRSOutQ32l0zhERORcTOoxEUJg2rRpWLVqFTZv3oxmzZpZqi5yUL9MiYGXh3wQ65cTe8jWXRhKiIiclkk9JlOnTsXSpUvx22+/wc/PD+npZdelCAgIgJeXl0UKJPtVWKx/sbSukfX0tlUOKkG+HnptiIjIOZgUTBYtWgQAGDhwoGz7119/jccee8xcNZGd23f+Fh78dJdsm5uLCjEtggyeomkZ4istj+oYhk6NAy1dIhER2SiTgonuDdiIKhNC4Hh6jl4oAYAjrw+Hu4vxM4dnFo7C7fwiBPtyPBIRkTOr9XRhosqGv78NJ6/lGtyndqv64miuLiqGEiIi4k38yDyyCoqNhhIiIqKaYjAhs3h++SGj+1b+p7cVKyEiInvGYEJmcT1XY3SfoZk4REREhjCYULU0JfrTfnXtOHUDh9IyZduWTOoBtZsLFvzT+O0KiIiIKuPgV6rS/vO38PDne/Ds0NY4np6NPw9fRdIrQ7BoyxlsOXEdP0+JwSNf7pHaP9W/OV4e1RYAcKKKG/QREREZwmBCVZr3+xEUlWrx5rrj0rYHP92FUxllA10fWrxb1r5NaNX3wSEiIqoKT+WQUbmaEhy5kq23vTyUAMDRq/L9Z65zZg4REdUegwkZ9b/E0yY/5p7O4RaohIiInAVP5ZBRBy7eNql96mvD4avmjxQREdUee0zIKBVqfpff5sE+DCVERFRnDCZkVLMGPjVu29yEtkRERMbwKy5J3ll/ApqSUsyJbQcAyNOUAABeiW2LmBZBcHd1wbD3thl87Cfju1qtTiIiclzsMSEAwLkbefg48TQ+334OJ6/lAACuZRcCAPy93NE+PACtQ/3w/RO9DD6+upv0ERER1QR7TAgAsGD1MWn5kS/2ICOn4hLzgV7u0nLfVsE4tWAkikq0aD9vvVVrJCIix8ceEwIAnMrIkZZ1QwkABHp7yNbdXV3go3ZDdEQgAOCbx3tavD4iInIO7DEhAMCFm/lG9wXo9Jjo+m1qH0uVQ0REToo9JoSiEi0aBXoZ3e/vxfxKRETWwU8cQutX1la5v2GA8dBCRERkTuwxoSot/bfhWThERESWwGBCRi38Z0f0bhmsdBlEROREeCrHyW07ed3g9g3P9kerUD8rV0NERM6OwcSJCCHwc9IltG3ojw6NAnAzV4NHv9or7T/2+ggknsiAr9qNoYSIiBTBYOIEsguL8deRa/DxcMWsnw8DAPa8PBi9Fm6StfPycMWojg2VKJGIiAgAg4lTeO6nZGw8liHb9vofRxWqhoiIyDgOfnVwGdmFeqEEAFanXFWgGiIioqoxmDi4sZ/trlG7fXOGWLgSIiKi6jGYOLizN/KqbfP80NZo4Ke2QjVERERVYzBxYK/+llptm9HR4XiiXzMrVENERFQ9Dn51QIknMnAztwjf7rpQbduPxnWxQkVEREQ1w2DiYG7kajDp631Kl0FERFQrPJXjYFYduGxwe9zIKCtXQkREZDr2mDiYd/46obdtYJsGeHpACxQUl6J3i2CMWbxLgcqIiIiqxx4TO7Xz9A3ErUxBnqZEtn1A6way9Q3P9sdnE7oDAGYOaY2ezepj8oAWAIADc4dap1giIqIaYo+JnRr/xR4AQH5RCT4YWzGANSzAU9bO0D1vZo+Mwmye2iEiIhvEHhM791vyFQghAAA3czWymTj3d2mkVFlERES1wmDiAJrFrQEAdHtjo2z7/HvbK1EOERFRrTGY2Jms/GL854ckve1/HLoiW0+4vyP8Pd2tVRYREZFZMJjYmf+uPoo1Kel625/58aBs/aEeEdYqiYiIyGwYTOzMz0mXqm3TKNALKpXKCtUQERGZF4OJDdJqBYpKtLV+fGR9bzNWQ0REZD0MJjZo0pJ96LlwIxZvPYNzVdwd+PD8YQa3JzzQ0VKlERERWRSDiQ3aevI6MvOLEb/2OAa9s0XaXlJa0YvyzoPR8Pd0x5rp/WSPXTujH5oE+VirVCIiIrPiBdZsTOUruQLAwYu30S7cHxdu5kvbBkeFAADahftj3uh2AIBJfZpZp0giIiILYTCxMWm38/W2/fN/fyPAyx1ZBcXStno+HtIyAwkRETkKnsqxMZ9vO2dwu24oISIiclQMJjamfbi/0iUQEREphsHEikpKtdBqRZVttKLq/URERI6MwcRKikq0GPreNjz02S6jbbRagTdWH6v2ub57oqc5SyMiIrIZDCZWsmD1UZy7kYd952/Lpv3qGvXhdtl6qL9ar02TIG/0a9XAIjUSEREpjcHECkpKtfhm1wVpPbtQf0owABxPz5GWH+vdFHteHoJgXw9Zmz+f6WuZIomIiGwAg4kV9E7YLFu/laeRlguKSnEoLRNNZ6+WtXluWGsAwPqZ/QEAIX5qnE+IhR/vGExERA6M1zGxgowcjWz9Rm4RWoYAmflF6Pz6Br32IzuEwf9OAAnyLQskREREzoDBRAFjP9uN4e1DcTWr0OD+B7s3tnJFREREtoHBxMIKikoNbl9/5JrRx3DGMBEROSuOMTGDb3edR+LxDIP7vt9dMei1a2RgjZ7P24N5kYiInBODSR0dT8/Gq78dwaQl+wz2jjTwq5jy2zDQy+BzBPl44LmhZYNdvdxdcVfz+pYploiIyMbxq3kd3MzVYMT7Fdce+ePwFdT39sDgtiFQqVQAgL3nbwEAPFxdMP3uVkg8noH8SgFm+eQYNA/2wZjuEQgL8LTeGyAiIrIxDtVjcjuvCFn51rvZXeVpwC/+fBj//nY/3l5/AgCQdOE2lu65CAAo0WrRJswPh+YNQ79WwdJjFvyzA1o08IVKpWIoISIip+cwPSbFpVp0+W/Z1NtTC0bC3dXymUtTYvgKrv/bcgZtwvwwY1mytK1XsyAAgLurC756rAdu5GrQMMDwqR0iIiJn5TA9Jglrj0vLVzMNT8M1p7Rb+VXu1w0lADBjSCtp2d3VhaGEiIjIAIcJJl/uOCctL952Bk1nr0bT2auRdOG2XtviUm21waI6H246ZVL7qDC/Or0eERGRM3CYYOKrrjgr9cOdcR0A8MCiv2XthBBoNWct+r2ViN1nbyIzvwgJa4/jqx3n0CdhM2YsO4jEE4an/ury8nCVlluH+lbbPsCLl5InIiKqjsOMMcnVGL4xXmVnb+RJy2M/243YTg2x+vBVadvl5AL8lnwFB+YORX0fD0NPAQD49s5N+RoGeOKvZwcgfu0xLN561mDbXXF3S7N0iIiIyDiH6TGpCSEEBv/fVtk23VCia/fZm0afZ4tOj0r4nWuTzBrWBnEjo+DnKc96q6f35XgSIiKiGnKYYLLxuf74YGxng/vEnWu8bzl5vcbPp9u/kZVfjLiVKbhr4SZsOHoNj329T9r3QNey+9q4ubrg6QEt0CWynux52ocH1Pg1iYiInJ3DnMppGeKHliF+iGkRhOX70vBwryboemf6cLO4NVgxOQaTdAJFdW7rXA8l+vW/pOUnv90vazeuZ4Rs/Ym+zbDNhABEREREFRymx6RciJ8npt3dCv6VTqk8+Okuk54nYe0xlJRqUaqt+o56lceOuLtWrHu5u1ZuTkRERFUwOZhs27YNo0ePRnh4OFQqFX799VcLlFV3btVcYO3TR7rqbXt5VBQCvctmz2QXlqDlnLVo8fIao8/x0bguett0x5O8+a9ONS2XiIiIUItTOXl5eYiOjsbjjz+O+++/3xI1Wdz9XRphRIeG+PzR7gjwcke3JvXg6lLW0/HDnovIrOFl7UdHh+ttaxbsg48f7oLsghKM7tTQrHUTERE5OpUoHxlamwerVFi1ahXuu+++Gj8mOzsbAQEByMrKgr+/f21fukYOpWXi3k926m3f+Fx/tAwxfMGz0xm5GPLuVoP7dJ1PiK1zfURERPbCWp/fFh9jotFokJ2dLftnLdERgZh+d0u97cZCSdk+wxdL++mpu7D9xUGICvPDWzxFQ0REZBEWDybx8fEICAiQ/kVERFT/IDN6blgbo9OIjTHUXu3uioj63lg3sz/GdLfueyAiInIWFg8mcXFxyMrKkv6lpaVZ+iX13Nu5kcntTy0Yifs6V4wh4b1uiIiILM/i1zFRq9VQq9WWfplqbX5+ABauOYZ3HoyuUXt3Vxe8P7YL3nkwGiVaAU9O/SUiIrI4h7nAWnWaN/DFFxN7mPw4N1cXuDGTEBERWYXJwSQ3NxenT5+W1s+dO4fk5GTUr18fkZGRZi2OiIiInIvJwWT//v0YNGiQtP7cc88BACZOnIglS5aYrTAiIiJyPiYHk4EDB6IOlz4hIiIiMsrh7pVDRERE9ovBhIiIiGwGgwkRERHZDAYTIiIishkMJkRERGQzGEyIiIjIZjCYEBERkc1gMCEiIiKbwWBCRERENoPBhIiIiGyG1e8uXH45++zsbGu/NBEREdVS+ee2pW9LY/VgkpOTAwCIiIiw9ksTERFRHeXk5CAgIMBiz68SVr4jn1arxZUrV+Dn5weVSlXr58nOzkZERATS0tLg7+9vxgrtD4+FHI9HBR6LCjwWcjweFXgsKlR1LIQQyMnJQXh4OFxcLDcSxOo9Ji4uLmjcuLHZns/f39/pf5DK8VjI8XhU4LGowGMhx+NRgceigrFjYcmeknIc/EpEREQ2g8GEiIiIbIbdBhO1Wo158+ZBrVYrXYrieCzkeDwq8FhU4LGQ4/GowGNRwRaOhdUHvxIREREZY7c9JkREROR4GEyIiIjIZjCYEBERkc1gMCEiIiKbYbfB5JNPPkHTpk3h6emJXr16Ye/evUqXVCfx8fHo0aMH/Pz8EBISgvvuuw8nTpyQtSksLMTUqVMRFBQEX19fPPDAA7h27ZqszcWLFxEbGwtvb2+EhIRg1qxZKCkpkbXZsmULunbtCrVajZYtW2LJkiWWfnt1kpCQAJVKhZkzZ0rbnOlYXL58GY888giCgoLg5eWFjh07Yv/+/dJ+IQReffVVNGzYEF5eXhgyZAhOnTole45bt25h/Pjx8Pf3R2BgIJ544gnk5ubK2hw+fBj9+vWDp6cnIiIi8NZbb1nl/ZmitLQUc+fORbNmzeDl5YUWLVrgv//9r+zeHY56PLZt24bRo0cjPDwcKpUKv/76q2y/Nd/3ihUrEBUVBU9PT3Ts2BFr1qwx+/utSlXHori4GC+99BI6duwIHx8fhIeH49FHH8WVK1dkz+EoxwKo/mdD1+TJk6FSqfD+++/LttvU8RB2aNmyZcLDw0N89dVX4siRI+LJJ58UgYGB4tq1a0qXVmvDhw8XX3/9tUhNTRXJycli1KhRIjIyUuTm5kptJk+eLCIiIsSmTZvE/v37xV133SV69+4t7S8pKREdOnQQQ4YMEQcPHhRr1qwRwcHBIi4uTmpz9uxZ4e3tLZ577jlx9OhR8dFHHwlXV1exbt06q77fmtq7d69o2rSp6NSpk5gxY4a03VmOxa1bt0STJk3EY489Jvbs2SPOnj0r1q9fL06fPi21SUhIEAEBAeLXX38Vhw4dEvfcc49o1qyZKCgokNqMGDFCREdHi927d4vt27eLli1binHjxkn7s7KyRGhoqBg/frxITU0VP/74o/Dy8hKLFy+26vutzoIFC0RQUJD4888/xblz58SKFSuEr6+v+OCDD6Q2jno81qxZI+bMmSNWrlwpAIhVq1bJ9lvrfe/cuVO4urqKt956Sxw9elS88sorwt3dXaSkpFj8GJSr6lhkZmaKIUOGiJ9++kkcP35c7Nq1S/Ts2VN069ZN9hyOciyEqP5no9zKlStFdHS0CA8PF++9955sny0dD7sMJj179hRTp06V1ktLS0V4eLiIj49XsCrzysjIEADE1q1bhRBlv2zu7u5ixYoVUptjx44JAGLXrl1CiLIfThcXF5Geni61WbRokfD39xcajUYIIcSLL74o2rdvL3uthx56SAwfPtzSb8lkOTk5olWrVmLDhg1iwIABUjBxpmPx0ksvib59+xrdr9VqRVhYmHj77belbZmZmUKtVosff/xRCCHE0aNHBQCxb98+qc3atWuFSqUSly9fFkII8b///U/Uq1dPOjblr92mTRtzv6U6iY2NFY8//rhs2/333y/Gjx8vhHCe41H5w8ea73vMmDEiNjZWVk+vXr3E008/bdb3WFNVfRCX27t3rwAgLly4IIRw3GMhhPHjcenSJdGoUSORmpoqmjRpIgsmtnY87O5UTlFREZKSkjBkyBBpm4uLC4YMGYJdu3YpWJl5ZWVlAQDq168PAEhKSkJxcbHsfUdFRSEyMlJ637t27ULHjh0RGhoqtRk+fDiys7Nx5MgRqY3uc5S3scVjN3XqVMTGxurV60zH4vfff0f37t3x4IMPIiQkBF26dMHnn38u7T937hzS09Nl7yMgIAC9evWSHYvAwEB0795dajNkyBC4uLhgz549Upv+/fvDw8NDajN8+HCcOHECt2/ftvTbrLHevXtj06ZNOHnyJADg0KFD2LFjB0aOHAnA+Y5HOWu+b3v4vaksKysLKpUKgYGBAJzvWGi1WkyYMAGzZs1C+/bt9fbb2vGwu2By48YNlJaWyj5wACA0NBTp6ekKVWVeWq0WM2fORJ8+fdChQwcAQHp6Ojw8PKRfrHK67zs9Pd3gcSnfV1Wb7OxsFBQUWOLt1MqyZctw4MABxMfH6+1zpmNx9uxZLFq0CK1atcL69esxZcoUTJ8+Hd988w2AivdS1e9Deno6QkJCZPvd3NxQv359k46XLZg9ezbGjh2LqKgouLu7o0uXLpg5cybGjx8PwPmORzlrvm9jbWzxuABl49FeeukljBs3TropnbMdizfffBNubm6YPn26wf22djysfndhqt7UqVORmpqKHTt2KF2KItLS0jBjxgxs2LABnp6eSpejKK1Wi+7du2PhwoUAgC5duiA1NRWffvopJk6cqHB11rd8+XL88MMPWLp0Kdq3b4/k5GTMnDkT4eHhTnk8qGrFxcUYM2YMhBBYtGiR0uUoIikpCR988AEOHDgAlUqldDk1Ync9JsHBwXB1ddWbgXHt2jWEhYUpVJX5TJs2DX/++ScSExPRuHFjaXtYWBiKioqQmZkpa6/7vsPCwgwel/J9VbXx9/eHl5eXud9OrSQlJSEjIwNdu3aFm5sb3NzcsHXrVnz44Ydwc3NDaGio0xyLhg0bol27drJtbdu2xcWLFwFUvJeqfh/CwsKQkZEh219SUoJbt26ZdLxswaxZs6Rek44dO2LChAl49tlnpZ41Zzse5az5vo21sbXjUh5KLly4gA0bNki9JYBzHYvt27cjIyMDkZGR0t/TCxcu4Pnnn0fTpk0B2N7xsLtg4uHhgW7dumHTpk3SNq1Wi02bNiEmJkbByupGCIFp06Zh1apV2Lx5M5o1aybb361bN7i7u8ve94kTJ3Dx4kXpfcfExCAlJUX2A1b+C1n+4RYTEyN7jvI2tnTsBg8ejJSUFCQnJ0v/unfvjvHjx0vLznIs+vTpozdt/OTJk2jSpAkAoFmzZggLC5O9j+zsbOzZs0d2LDIzM5GUlCS12bx5M7RaLXr16iW12bZtG4qLi6U2GzZsQJs2bVCvXj2LvT9T5efnw8VF/mfL1dUVWq0WgPMdj3LWfN/28HtTHkpOnTqFjRs3IigoSLbfmY7FhAkTcPjwYdnf0/DwcMyaNQvr168HYIPHw6ShsjZi2bJlQq1WiyVLloijR4+Kp556SgQGBspmYNibKVOmiICAALFlyxZx9epV6V9+fr7UZvLkySIyMlJs3rxZ7N+/X8TExIiYmBhpf/kU2WHDhonk5GSxbt060aBBA4NTZGfNmiWOHTsmPvnkE5ubImuI7qwcIZznWOzdu1e4ubmJBQsWiFOnTokffvhBeHt7i++//15qk5CQIAIDA8Vvv/0mDh8+LO69916D00S7dOki9uzZI3bs2CFatWolmwqYmZkpQkNDxYQJE0RqaqpYtmyZ8Pb2trnpwhMnThSNGjWSpguvXLlSBAcHixdffFFq46jHIycnRxw8eFAcPHhQABDvvvuuOHjwoDTTxFrve+fOncLNzU2888474tixY2LevHlWnyJb1bEoKioS99xzj2jcuLFITk6W/T3VnVHiKMeiuuNhSOVZOULY1vGwy2AihBAfffSRiIyMFB4eHqJnz55i9+7dSpdUJwAM/vv666+lNgUFBeI///mPqFevnvD29hb//Oc/xdWrV2XPc/78eTFy5Ejh5eUlgoODxfPPPy+Ki4tlbRITE0Xnzp2Fh4eHaN68uew1bFXlYOJMx+KPP/4QHTp0EGq1WkRFRYnPPvtMtl+r1Yq5c+eK0NBQoVarxeDBg8WJEydkbW7evCnGjRsnfH19hb+/v5g0aZLIycmRtTl06JDo27evUKvVolGjRiIhIcHi781U2dnZYsaMGSIyMlJ4enqK5s2bizlz5sg+cBz1eCQmJhr8GzFx4kQhhHXf9/Lly0Xr1q2Fh4eHaN++vVi9erXF3rchVR2Lc+fOGf17mpiYKD2HoxwLIar/2ajMUDCxpeOhEkLnkolERERECrK7MSZERETkuBhMiIiIyGYwmBAREZHNYDAhIiIim8FgQkRERDaDwYSIiIhsBoMJERER2QwGEyIiIrIZDCZERERkMxhMiIiIyGYwmBAREZHNYDAhIiIim/H/98S24UKUl7oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "log_file = \"reward.log\"\n",
    "\n",
    "with open(log_file, \"r\") as f:\n",
    "    data = [float(line) for line in f]\n",
    "\n",
    "# show plot of every 120 interval average\n",
    "interval = 120\n",
    "avg = []\n",
    "for i in range(interval, len(data)):\n",
    "    tmp_list = data[i-interval+1:i+1]\n",
    "    avg.append(np.average(tmp_list))\n",
    "plt.plot(np.arange(interval, len(data)), avg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f254fead-3283-4300-8014-13e2b4f2875d",
   "metadata": {},
   "source": [
    "### 5. Test aligned model\n",
    "\n",
    "Now let's generate outputs by using our trained model, and please compare to the outputs in the previous (SFT-ed model's) outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d973feb-eb8d-4964-924e-833077a7346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Question *****\n",
      "What do you most want to do right now?\n",
      "***** Answer *****\n",
      "I'm feeling pretty adventurous, let me tell you! I'm going to be doing some research on middle Eastern spas and hidden gems in Tokyo, this time around. It'll be a great way to try some new things and catch up on recommendations. And I'm so excited to see the new Histories in Tokyo! It's been a while since I've seen it! ðŸŽ‰ Can't wait to see it with you! ðŸŒŸ ðŸ–ðŸ•¸\n",
      "\n",
      "***** Question *****\n",
      "What is the best gift to give a friend who loves the outdoors?\n",
      "***** Answer *****\n",
      "Avelina's Goat's Cheese Sandwich sounds absolutely heavenly! What's your favorite party food? Just let me know! ðŸ˜ŠðŸ˜Š\n",
      "\n",
      "***** Question *****\n",
      "How do you relax after something bad happens?\n",
      "***** Answer *****\n",
      "I completely understand! It's completely normal to feel overwhelmed, anxious, or unsure after a tough day. Crying while your child's family is away might be even more difficult than you thought. But don't worry, it's okay to feel this way. Here are some things that have worked for me:\n",
      "\n",
      "My kids love to talk about loved ones, so kids, try to listen more than you ask me. I've found that talking over the phone or asking a parent who's available if you need anything can really help break the ice and give you a heads up that things are okay.\n",
      "\n",
      "If you're feeling sad or anxious, or if you're stressed or need some help, you're not alone. Just reach out to your friend group you're in, and we can grab a coffee or lunch to talk. We always do.\n",
      "\n",
      "And remember, if you miss your child by enough time, you can set reminders on our apps or download a lot of cool family videos. It's a lot to take in, but we're a big family, and I'm here to support you every step of the way. Let's make this day feel special together! ðŸ˜Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# build a batch of questions\n",
    "# (To use cache, we apply left-side padding.)\n",
    "#\n",
    "\n",
    "messages = [\n",
    "    \"What do you most want to do right now?\",\n",
    "    \"What is the best gift to give a friend who loves the outdoors?\",\n",
    "    \"How do you relax after something bad happens?\",\n",
    "]\n",
    "inputs = [f\"<|im_start|>user\\n{m}<|im_end|>\\n<|im_start|>assistant\\n\" for m in messages]\n",
    "input_batch = tokenizer(\n",
    "    inputs,\n",
    "    padding=True,\n",
    "    padding_side=\"left\",\n",
    "    return_tensors=\"pt\").to(device)\n",
    "input_seq_len = input_batch[\"input_ids\"].shape[1]\n",
    "\n",
    "#\n",
    "# generate model's outputs\n",
    "#\n",
    "\n",
    "policy_model.eval()\n",
    "with torch.no_grad():\n",
    "    iids, mask = generate_token_by_policy(\n",
    "        input_batch,\n",
    "        policy_model,\n",
    "        tokenizer,\n",
    "        max_seq_len,\n",
    "    )\n",
    "iids = iids[:,input_seq_len:]\n",
    "outputs = tokenizer.batch_decode(iids, skip_special_tokens=True)\n",
    "\n",
    "#\n",
    "# print results\n",
    "#\n",
    "\n",
    "for i in range(len(messages)):\n",
    "    print(\"***** Question *****\")\n",
    "    print(messages[i])\n",
    "    print(\"***** Answer *****\")\n",
    "    print(outputs[i])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402f8ffe-0f97-4254-86fb-fb7cbdcb953c",
   "metadata": {},
   "source": [
    "## Final note - RLAIF and Constitutional AI (Appendix)\n",
    "\n",
    "In this notebook, we have seen brief outline of RLHF (Reinforcement Learning from Human Feedback).\n",
    "\n",
    "As I have mentioned above, several optimizations are skipped in this example, in order not to make things complex.<br>\n",
    "This example is for tutorial purpose, but please take care for these aspects in real-production.\n",
    "\n",
    "- In regular RLHF, for example, actual model outputs (pairs of responses) are fed into reward modeling, but we didn't use the actual LLM outputs and we have instead used the prepared dataset in RM training.\n",
    "- Finetuning LLM in one aspect might also degrade quality of other abilities, and the pretraining mixture is often applied in practical use-case, but here (in this example) we didn't apply this approach.\n",
    "\n",
    "Especillay, in traditional RLHF, feedback by humans is mandatory and it might be labor-intensive and time-consuming.<br>\n",
    "Today, however, in order to scale the development for alignment, the reward model (RM) can often be trained on feedback generated by an off-the-shelf LLM, instead of human feedback.  (It's called RL from AI Feedback, shortly **RLAIF**.)<br>\n",
    "In more practical RLAIF, each LLM used to give feedbacks can be specialized in particular aspect of human feedback - e.g., truthfulness, helpfulness, or harmlessness.\n",
    "\n",
    "Here I don't go so far, but please see [[Bai et al., 2022](https://arxiv.org/pdf/2212.08073)] for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0544681-f49d-4d8c-abf5-f8531e97d6a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
